{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# Notebook Pelatihan Utama - Pengenalan Ucapan Disartria (KAGGLE VERSION)\n",
    "**Version:** 20260115_2356\\n",
    "\n",
    "**Tujuan:** Analisis Perbandingan **Lightweight CNN-STFT** (Diusulkan) vs **Model Transfer Learning**.\n",
    "**Platform:** Kaggle Kernels (GPU T4 x2).\n",
    "**Strategy:** Subject-Independent Split (Verified).\n",
    "\n",
    "## \ud83c\udd95 Log Perubahan (Changelog)\n",
    "\n*   **Fix**: Resolved `axis 1 out of bounds` error for NASNetMobile (Robust 1D/2D output handling).\n*   **Fix**: Added `%load_ext tensorboard` magic command to ensure TensorBoard works in Kaggle.\n*   **Feature**: Added **Thesis-Ready Visualization** suite (CM, ROC, PRC, Learning Curves saved as PNG).\n*   **Feature**: Added **Comparison Plots** (Accuracy & Time Bar Charts) at the end of the notebook.\n*   **Feature**: Added **Extended CSV Logs** (Predictions with file names for Error Analysis).\n*   **Optimization**: Fully implemented **Paper 2 Alignment** (16kHz, Librosa STFT).\n\\n",
    "\n",
    "## \ud83d\udccb Panduan Setup Kaggle\n",
    "1. **Add Data**: Upload folder `backend` anda sebagai Dataset (beri nama `thesis-backend` misalnya).\n",
    "2. **Add Data**: Cari dataset `UASpeech` dan `TORGO` (atau upload zip-nya jika punya privasi).\n",
    "3. **Internet**: Aktifkan Internet di menu Settings (kanan) jika perlu download via `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env_setup"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Environment & Path (Kaggle Symlink Fix)\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "print(\"\ud83d\ude80 Memulai Setup Kaggle Environment...\")\n",
    "\n",
    "# A. Cari file 'config.py' dimanapun dia berada\n",
    "config_path = None\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    if 'config.py' in files:\n",
    "        config_path = os.path.join(root, 'config.py')\n",
    "        break\n",
    "\n",
    "if config_path:\n",
    "    print(f\"\u2705 Ditemukan Config di: {config_path}\")\n",
    "    source_dir = os.path.dirname(config_path)\n",
    "    \n",
    "    # B. Buat Symlink 'src' di Working Directory\n",
    "    # Tujuannya agar 'from src import config' SELALU jalan, tidak peduli struktur aslinya rusak/flatten\n",
    "    target_link = '/kaggle/working/src'\n",
    "    if os.path.exists(target_link):\n",
    "        if os.path.islink(target_link):\n",
    "            os.unlink(target_link)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(target_link)\n",
    "            \n",
    "    os.symlink(source_dir, target_link)\n",
    "    print(f\"\ud83d\udd17 Symlink dibuat: {target_link} -> {source_dir}\")\n",
    "    \n",
    "    # C. Tambah Working Dir ke Sys Path\n",
    "    if '/kaggle/working' not in sys.path:\n",
    "        sys.path.append('/kaggle/working')\n",
    "else:\n",
    "    print(\"\u274c FATAL: File 'config.py' tidak ditemukan di Input manapun!\")\n",
    "    print(\"   Pastikan Anda sudah 'Add Data' folder backend.\")\n",
    "\n",
    "# D. Setup Output Paths\n",
    "OUTPUT_ROOT = '/kaggle/working'\n",
    "LOCAL_DATA_ROOT = '/kaggle/working/data'\n",
    "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Environment Siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 2. Install Dependencies\n",
    "!pip install -q tensorflow-io\n",
    "!pip install -q pandas matplotlib seaborn scikit-learn librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# 3. Import Modul Proyek\n",
    "try:\n",
    "    from src import config, data_loader, models, trainer\n",
    "    print(\"\u2705 Modul berhasil diimport: config, data_loader, models, trainer\")\n",
    "\n",
    "    # Override Config untuk Kaggle Output\n",
    "    config.MODELS_DIR = os.path.join(OUTPUT_ROOT, 'models')\n",
    "    config.OUTPUTS_DIR = os.path.join(OUTPUT_ROOT, 'outputs')\n",
    "    os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(config.OUTPUTS_DIR, exist_ok=True)\n",
    "    print(f\"\ud83d\udcc2 Output Directory set to: {config.OUTPUTS_DIR}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Gagal import modul: {e}\")\n",
    "    print(\"Pastikan 'backend' terdeteksi dengan benar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_helpers"
   },
   "outputs": [],
   "source": [
    "# 3.5 Visualization Helpers (From Paper 2 - CLONED)\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, classification_report\\n",
    "import numpy as np\\n",
    "import pandas as pd\\n",
    "import os\\n",
    "\\n",
    "def smooth_curve(points, factor=0.8):\\n",
    "    \\\"\\\"\\\"Membuat kurva lebih halus menggunakan exponential moving average.\\\"\\\"\\\"\\n",
    "    smoothed = []\\n",
    "    for point in points:\\n",
    "        if smoothed:\\n",
    "            # Basic EMA\\n",
    "            smoothed.append(smoothed[-1] * factor + point * (1 - factor))\\n",
    "        else:\\n",
    "            smoothed.append(point)\\n",
    "    return smoothed\\n",
    "\\n",
    "def plot_learning_curve(history, model_name, run_name):\\n",
    "    # 1. Grafik Kurva Pembelajaran (ASLI / TANPA SMOOTHING)\\n",
    "    sns.set_style(\\\"whitegrid\\\")\\n",
    "    fig_learning_asli, axs_learning_asli = plt.subplots(1, 2, figsize=(15, 5))\\n",
    "    fig_learning_asli.suptitle(f'Kurva Pembelajaran (Asli): {model_name}', fontsize=16)\\n",
    "\\n",
    "    axs_learning_asli[0].plot(history['accuracy'], '-', label='Akurasi Training', linewidth=2)\\n",
    "    axs_learning_asli[0].plot(history['val_accuracy'], '-', label='Akurasi Validasi', linewidth=2)\\n",
    "    axs_learning_asli[0].set_title('Grafik Akurasi (Asli)')\\n",
    "    axs_learning_asli[0].set_xlabel('Epoch'); axs_learning_asli[0].set_ylabel('Akurasi')\\n",
    "    axs_learning_asli[0].legend(loc='lower right')\\n",
    "\\n",
    "    axs_learning_asli[1].plot(history['loss'], '-', label='Loss Training', linewidth=2)\\n",
    "    axs_learning_asli[1].plot(history['val_loss'], '-', label='Loss Validasi', linewidth=2)\\n",
    "    axs_learning_asli[1].set_title('Grafik Loss (Asli)')\\n",
    "    axs_learning_asli[1].set_xlabel('Epoch'); axs_learning_asli[1].set_ylabel('Loss')\\n",
    "    axs_learning_asli[1].legend(loc='upper right')\\n",
    "    \\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_learning_asli.png\\\"))\\n",
    "    plt.show()\\n",
    "\\n",
    "    # 2. Grafik Kurva Pembelajaran (DENGAN SMOOTHING)\\n",
    "    fig_learning_smooth, axs_learning_smooth = plt.subplots(1, 2, figsize=(15, 5))\\n",
    "    fig_learning_smooth.suptitle(f'Kurva Pembelajaran (Smoothed): {model_name}', fontsize=16)\\n",
    "\\n",
    "    smoothed_accuracy = smooth_curve(history['accuracy'])\\n",
    "    smoothed_val_accuracy = smooth_curve(history['val_accuracy'])\\n",
    "    smoothed_loss = smooth_curve(history['loss'])\\n",
    "    smoothed_val_loss = smooth_curve(history['val_loss'])\\n",
    "\\n",
    "    axs_learning_smooth[0].plot(history['accuracy'], '-', label='Akurasi Training (Asli)', alpha=0.3)\\n",
    "    axs_learning_smooth[0].plot(history['val_accuracy'], '-', label='Akurasi Validasi (Asli)', alpha=0.3)\\n",
    "    axs_learning_smooth[0].plot(smoothed_accuracy, '-', label='Akurasi Training (Smoothed)', linewidth=2)\\n",
    "    axs_learning_smooth[0].plot(smoothed_val_accuracy, '-', label='Akurasi Validasi (Smoothed)', linewidth=2)\\n",
    "    axs_learning_smooth[0].set_title('Grafik Akurasi (Smoothed)')\\n",
    "    axs_learning_smooth[0].set_xlabel('Epoch'); axs_learning_smooth[0].set_ylabel('Akurasi')\\n",
    "    axs_learning_smooth[0].legend(loc='lower right')\\n",
    "\\n",
    "    axs_learning_smooth[1].plot(history['loss'], '-', label='Loss Training (Asli)', alpha=0.3)\\n",
    "    axs_learning_smooth[1].plot(history['val_loss'], '-', label='Loss Validasi (Asli)', alpha=0.3)\\n",
    "    axs_learning_smooth[1].plot(smoothed_loss, '-', label='Loss Training (Smoothed)', linewidth=2)\\n",
    "    axs_learning_smooth[1].plot(smoothed_val_loss, '-', label='Loss Validasi (Smoothed)', linewidth=2)\\n",
    "    axs_learning_smooth[1].set_title('Grafik Loss (Smoothed)')\\n",
    "    axs_learning_smooth[1].set_xlabel('Epoch'); axs_learning_smooth[1].set_ylabel('Loss')\\n",
    "    axs_learning_smooth[1].legend(loc='upper right')\\n",
    "\\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_learning_smooth.png\\\"))\\n",
    "    plt.show()\\n",
    "\\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, model_name, run_name):\\n",
    "    # Paper 2 Style: Heatmap with Count + Percentage\\n",
    "    cm = confusion_matrix(y_true, y_pred)\\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\\n",
    "    annot_labels = (np.asarray([\\\"{0:d}\\\\n({1:.1%})\\\".format(value, P_value)\\n",
    "                                  for value, P_value in zip(cm.flatten(), cm_percent.flatten())])\\n",
    "                    ).reshape(cm.shape)\\n",
    "    plt.figure(figsize=(8, 6))\\n",
    "    sns.heatmap(cm_percent, annot=annot_labels, fmt='', cmap='Blues',\\n",
    "                xticklabels=classes, yticklabels=classes)\\n",
    "    plt.title(f'Confusion Matrix: {model_name}', fontsize=14)\\n",
    "    plt.ylabel('Label Aktual'); plt.xlabel('Label Prediksi')\\n",
    "    plt.tight_layout()\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_cm.png\\\"))\\n",
    "    plt.show()\\n",
    "\\n",
    "def plot_roc_curve(y_true, y_pred_probs, model_name, run_name):\\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\\n",
    "    roc_auc = auc(fpr, tpr)\\n",
    "    plt.figure(figsize=(8, 6))\\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\\n",
    "    plt.legend(loc=\\\"lower right\\\")\\n",
    "    plt.title(f'ROC: {model_name}')\\n",
    "    plt.grid(True)\\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_roc.png\\\"))\\n",
    "    plt.close()\\n",
    "\\n",
    "def plot_pr_curve(y_true, y_pred_probs, model_name, run_name):\\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs)\\n",
    "    plt.figure(figsize=(8, 6))\\n",
    "    plt.plot(recall, precision, color='purple', lw=2)\\n",
    "    plt.title(f'PR Curve: {model_name}')\\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\\n",
    "    plt.grid(True)\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_prc.png\\\"))\\n",
    "    plt.close()\\n",
    "\\n",
    "def plot_class_report(y_true, y_pred, classes, model_name, run_name):\\n",
    "    # Paper 2 Style: Detailed Bar Chart per Class\\n",
    "    report_dict = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\\n",
    "    df = pd.DataFrame(report_dict).transpose().drop(['accuracy', 'macro avg', 'weighted avg'])\\n",
    "    df = df.reset_index().rename(columns={'index':'class'}).melt(id_vars='class', value_vars=['precision','recall','f1-score'])\\n",
    "    \\n",
    "    plt.figure(figsize=(10, 6))\\n",
    "    ax = sns.barplot(x='class', y='value', hue='variable', data=df, palette='viridis')\\n",
    "    plt.title(f'Grafik Laporan Klasifikasi per Kelas: {model_name}', fontsize=14)\\n",
    "    plt.ylim(0, 1.1)\\n",
    "    plt.xlabel('Kelas'); plt.ylabel('Skor')\\n",
    "    plt.legend(title='Metrik')\\n",
    "    for p in ax.patches:\\n",
    "        ax.annotate(f\\\"{p.get_height():.2f}\\\", (p.get_x() + p.get_width() / 2., p.get_height()),\\n",
    "                      ha='center', va='center', xytext=(0, 9), textcoords='offset points')\\n",
    "                      \\n",
    "    plt.tight_layout()\\n",
    "    plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_report_bar.png\\\"))\\n",
    "    plt.close()\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_prep_func"
   },
   "outputs": [],
   "source": [
    "# 4. Persiapan Data (Kaggle Auto-Detect or Gdown)\n",
    "import shutil\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "# IDs Google Drive (Backup jika file tidak ada di Kaggle Dataset)\n",
    "UASPEECH_ID = '1L17F0SAkRk3rEjHDUyToLUvNp99sNMvE'\n",
    "TORGO_ID = '1YU7aCqa4qyn75XRdFPAWEqVv_1Qpl9cG'\n",
    "\n",
    "def setup_dataset_kaggle(name, file_id, extract_path):\n",
    "    print(f\"\\n--- Setup Dataset: {name} ---\")\n",
    "    \n",
    "    # 1. Cek di /kaggle/input (Siapa tau user sudah add data)\n",
    "    # Polanya: /kaggle/input/<name> atau /kaggle/input/<anything>/<name>\n",
    "    candidates = glob.glob(f'/kaggle/input/**/*{name}*', recursive=True)\n",
    "    \n",
    "    # Filter hanya folder yang valid (bukan file zip/meta)\n",
    "    potential_dirs = [c for c in candidates if os.path.isdir(c)]\n",
    "    \n",
    "    # Spesifik untuk TORGO/UASpeech foldernya biasanya 'UASpeech' atau 'TORGO'\n",
    "    for p in potential_dirs:\n",
    "        if os.path.basename(p).lower() == name.lower() or os.path.basename(p).lower() == f\"{name}_smalldataset\".lower():\n",
    "             print(f\"\u2705 Ditemukan Dataset di Input: {p}\")\n",
    "             return p\n",
    "\n",
    "    # 3. Jika tidak ketemu di Input, Coba Download (Gdown)\n",
    "    print(f\"\u26a0\ufe0f {name} tidak ditemukan di ke Kaggle Input. Mencoba download via Gdown...\")\n",
    "    \n",
    "    local_zip_path = os.path.join(extract_path, f\"{name}.zip\")\n",
    "    target_extract = os.path.join(extract_path, name)\n",
    "    \n",
    "    if os.path.exists(target_extract):\n",
    "         print(f\"\u2705 Dataset sudah ada di Working Dir: {target_extract}\")\n",
    "         return target_extract\n",
    "         \n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    gdown.download(url, local_zip_path, quiet=False)\n",
    "    \n",
    "    print(f\"Mengekstrak {name}...\")\n",
    "    subprocess.check_call(['unzip', '-o', '-q', local_zip_path, '-d', extract_path])\n",
    "    print(f\"\u2705 {name} Selesai diekstrak.\")\n",
    "    \n",
    "    # Handle nama folder TORGO yang kadang beda\n",
    "    if name == 'TORGO' and not os.path.exists(target_extract):\n",
    "         alt = os.path.join(extract_path, 'TORGO_smalldataset')\n",
    "         if os.path.exists(alt): return alt\n",
    "         \n",
    "    return target_extract\n",
    "\n",
    "# Jalankan Setup\n",
    "uaspeech_path = setup_dataset_kaggle('UASpeech', UASPEECH_ID, LOCAL_DATA_ROOT)\n",
    "torgo_path = setup_dataset_kaggle('TORGO', TORGO_ID, LOCAL_DATA_ROOT)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOADING DATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMemuat Path File...\")\n",
    "\n",
    "# Load Path File Audio\n",
    "uaspeech_files, uaspeech_labels, uaspeech_speakers = data_loader.get_file_paths(uaspeech_path, 'UASpeech')\n",
    "torgo_files, torgo_labels, torgo_speakers = data_loader.get_file_paths(torgo_path, 'TORGO')\n",
    "\n",
    "# --- GENERATE DATASET STATS FOR DASHBOARD ---\n",
    "import json\n",
    "print(\"Generating Dataset Statistics...\")\n",
    "\n",
    "def get_stats(name, files, labels, speakers):\n",
    "    unique_lbl = list(set(labels))\n",
    "    counts = {l: 0 for l in unique_lbl}\n",
    "    for l in labels: counts[l] += 1\n",
    "    \n",
    "    summary = []\n",
    "    for l in unique_lbl:\n",
    "        cat = \"Dysarthric\" if l == 1 else \"Control\"\n",
    "        total = counts[l]\n",
    "        summary.append({\n",
    "            \"category\": cat,\n",
    "            \"speakers\": len(set(speakers)), # Rough approx\n",
    "            \"totalRaw\": total,\n",
    "            \"trainRaw\": int(total * 0.8),\n",
    "            \"testRaw\": total - int(total * 0.8)\n",
    "        })\n",
    "        \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"stats\": {\n",
    "            \"samples\": f\"{len(files):,}\",\n",
    "            \"classes\": str(len(unique_lbl)),\n",
    "            \"avgLen\": \"N/A\" # Skip expensive calc\n",
    "        },\n",
    "        \"summaryData\": summary\n",
    "    }\n",
    "\n",
    "stats_export = {\n",
    "    \"uaspeech\": get_stats('UASpeech', uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    \"torgo\": get_stats('TORGO', torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "with open(os.path.join(config.OUTPUTS_DIR, \"dataset_stats.json\"), 'w') as f:\n",
    "    json.dump(stats_export, f, indent=4)\n",
    "print(\"\u2705 dataset_stats.json saved.\")\n",
    "\n",
    "# --- GENERATE REAL EDA SAMPLES (Audio + Signals) ---\n",
    "print(\"Generating EDA Samples (Waveform & Spectrogram data)...\")\n",
    "import random\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "samples_out_dir = os.path.join(config.OUTPUTS_DIR, \"samples\")\n",
    "os.makedirs(samples_out_dir, exist_ok=True)\n",
    "\n",
    "eda_export = {}\n",
    "\n",
    "# Iterate over both datasets\n",
    "for ds_name, (ds_files, ds_labels, ds_speakers) in [('uaspeech', (uaspeech_files, uaspeech_labels, uaspeech_speakers)), ('torgo', (torgo_files, torgo_labels, torgo_speakers))]:\n",
    "    eda_export[ds_name] = {'dysarthric': [], 'control': []}\n",
    "    \n",
    "    # Binary Classification Logic (Assuming 1=Dysarthric)\n",
    "    # Note: If labels are different, adjust accordingly.\n",
    "    # Based on previous cell: \"Dysarthric\" if l == 1 else \"Control\"\n",
    "    \n",
    "    indices_dys = [i for i, x in enumerate(ds_labels) if x == 1]\n",
    "    indices_ctrl = [i for i, x in enumerate(ds_labels) if x != 1]\n",
    "    \n",
    "    # Pick 5 random from each\n",
    "    picks_dys = random.sample(indices_dys, min(5, len(indices_dys)))\n",
    "    picks_ctrl = random.sample(indices_ctrl, min(5, len(indices_ctrl)))\n",
    "    \n",
    "    for category, picks in [('dysarthric', picks_dys), ('control', picks_ctrl)]:\n",
    "        for idx in picks:\n",
    "            src = ds_files[idx]\n",
    "            fname = f\"{ds_name}_{os.path.basename(src)}\" # Prefix to avoid collision\n",
    "            dst = os.path.join(samples_out_dir, fname)\n",
    "            shutil.copy(src, dst)\n",
    "            \n",
    "            # Analyze Signal\n",
    "            try:\n",
    "                y, sr = librosa.load(src, sr=16000)\n",
    "                duration = len(y) / sr\n",
    "                \n",
    "                # 1. Waveform (100 points max, absolute amplitude)\n",
    "                hop_len = max(1, len(y) // 80) # 80 bars\n",
    "                waveform = [float(np.max(np.abs(y[i:i+hop_len]))) for i in range(0, len(y), hop_len)][:80]\n",
    "                # Normalize waveform 0-100 for CSS height\n",
    "                max_val = max(waveform) if waveform else 1\n",
    "                waveform = [int((v / max_val) * 100) for v in waveform]\n",
    "                \n",
    "                # 2. Mel Spectrogram (Low Res for JSON: 40 bands x 60 time steps)\n",
    "                # Enough for visual \"texture\" without bloating JSON\n",
    "                n_mels = 40\n",
    "                hop_spec = len(y) // 60\n",
    "                if hop_spec < 512: hop_spec = 512 # Minimum hop\n",
    "                \n",
    "                S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_spec)\n",
    "                S_db = librosa.power_to_db(S, ref=np.max)\n",
    "                \n",
    "                # --- SAVE PNGs (Thesis Requirement - Matches Paper 2 Colors) ---\n",
    "                # Determine Color based on Dataset & Category\n",
    "                # Paper 2: UASpeech (Blue/Orange), TORGO (Green/Tomato)\n",
    "                color = 'blue' # Default\n",
    "                if ds_name.lower() == 'uaspeech':\n",
    "                    color = 'dodgerblue' if category == 'control' else 'orangered'\n",
    "                else: # TORGO\n",
    "                    color = 'mediumseagreen' if category == 'control' else 'tomato'\n",
    "\n",
    "                # 1. Waveform\n",
    "                plt.figure(figsize=(4, 2))\n",
    "                librosa.display.waveshow(y, sr=sr, alpha=0.7, color=color)\n",
    "                plt.title(f'Wave: {os.path.basename(fname)}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(samples_out_dir, f\\\"{os.path.splitext(fname)[0]}_wave.png\\\"))\n",
    "                plt.close()\n",
    "                \n",
    "                # 2. Spectrogram\n",
    "                plt.figure(figsize=(4, 2))\n",
    "                librosa.display.specshow(S_db, sr=sr, hop_length=hop_spec, x_axis='time', y_axis='mel')\n",
    "                plt.colorbar(format='%+2.0f dB')\n",
    "                plt.title(f'MelSpec: {os.path.basename(fname)}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(samples_out_dir, f\\\"{os.path.splitext(fname)[0]}_spec.png\\\"))\n",
    "                plt.close()\n",
    "                # --------------------------------------\n",
    "                \n",
    "                # Normalize 0-1\n",
    "                min_db, max_db = S_db.min(), S_db.max()\n",
    "                S_norm = (S_db - min_db) / (max_db - min_db)\n",
    "                \n",
    "                # Ensure dimensions (cut if too long)\n",
    "                if S_norm.shape[1] > 60: S_norm = S_norm[:, :60]\n",
    "                \n",
    "                spectrogram = S_norm.tolist() # List of lists\n",
    "                \n",
    "                eda_export[ds_name][category].append({\n",
    "                    \"id\": os.path.splitext(fname)[0],\n",
    "                    \"name\": fname,\n",
    "                    \"duration\": f\"{duration:.1f}s\",\n",
    "                    \"durationSec\": duration,\n",
    "                    \"type\": category,\n",
    "                    \"severity\": \"Unknown\",\n",
    "                    \"waveform\": waveform,\n",
    "                    \"spectrogram\": spectrogram,\n",
    "                    \"url\": f\"/static/samples/{fname}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Error processing {fname}: {e}\")\n",
    "\n",
    "with open(os.path.join(config.OUTPUTS_DIR, \"eda_samples.json\"), 'w') as f:\n",
    "    json.dump(eda_export, f)\n",
    "print(\"\u2705 eda_samples.json saved (Audio & Visuals).\")\n",
    "\n",
    "print(\"Data terload. Siap training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_analysis"
   },
   "outputs": [],
   "source": [
    "# 5. ANALISIS MODEL & PERBANDINGAN STRUKTUR (WAJIB PAPER 2)\n",
    "# Bagian ini dipisahkan agar analisa FLOPs, Parameter, dan Memory terlihat jelas sebelum Training dimulai.\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 2. Membangun dan Meringkas Semua Arsitektur Model ---\")\n",
    "summary_list = []\n",
    "\n",
    "# Setup Input Shape Standar untuk Analisa (3 Channel untuk Model TL, 1 Channel untuk STFT)\n",
    "# Fix: Gunakan MFCC_MAX_LEN yang benar dari config\n",
    "# UPDATED: ImageNet models need 3 channels (RGB emulation)\n",
    "input_shape_mfcc = (config.N_MFCC, config.MFCC_MAX_LEN, 3)\n",
    "# Fix: Hitung N_STFT dari N_FFT/2 + 1 (Spectrogram Height)\n",
    "n_stft_bins = (config.N_FFT // 2) + 1\n",
    "input_shape_stft = (n_stft_bins, config.MFCC_MAX_LEN, 1)\n",
    "\n",
    "for model_key, model_display_name in config.MODELS.items():\n",
    "    print(f\"Menganalisis arsitektur untuk: {model_display_name}...\")\n",
    "    \n",
    "    # Tentukan input shape berdasarkan jenis model\n",
    "    current_input_shape = input_shape_stft if model_key == 'cnn_stft' else input_shape_mfcc\n",
    "    \n",
    "    # Build Model\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        model = models.get_model(model_key, current_input_shape, num_classes=2)\n",
    "        \n",
    "        # Hitung Metrik\n",
    "        total_params = model.count_params()\n",
    "        # Hitung FLOPs\n",
    "        flops = trainer.get_flops(model)\n",
    "        peak_mem_32bit, disk_size_32bit = trainer.get_model_memory_usage(model)\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Gagal build/metric {model_display_name}: {e}\")\n",
    "        flops = 0; peak_mem_32bit = 0; disk_size_32bit = 0\n",
    "        # Dummy summary\n",
    "        architecture_summary = \"Error building model\"\n",
    "    else:\n",
    "        # Capture Summary\n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        architecture_summary = stream.getvalue()\n",
    "        stream.close()\n",
    "\n",
    "    summary_list.append({\n",
    "        \"Model\": model_display_name,\n",
    "        \"Total Parameter\": total_params,\n",
    "        \"FLOPs\": flops,\n",
    "        \"Ukuran di Disk (32-bit)\": disk_size_32bit,\n",
    "        \"Estimasi Ukuran 8-bit\": disk_size_32bit / 4,\n",
    "        \"Estimasi Memori Aktivasi 8-bit\": peak_mem_32bit / 4,\n",
    "        \"Architecture Summary\": architecture_summary\n",
    "    })\n",
    "\n",
    "    # --- SAVE EFFICIENCY METRICS (JSON) ---\n",
    "    efficiency_export = {}\n",
    "    for item in summary_list:\n",
    "        # Clean up keys for JSON export\n",
    "        efficiency_export[item['Model']] = {\n",
    "            \"params\": str(item['Total Parameter']),\n",
    "            \"flops\": str(item['FLOPs']),\n",
    "            \"size\": f\"{item['Estimasi Ukuran 8-bit'] / 1024:.2f} MB\",\n",
    "            \"activation\": f\"{item['Estimasi Memori Aktivasi 8-bit'] / 1024:.2f} KB\"\n",
    "        }\n",
    "    \n",
    "    with open(os.path.join(config.OUTPUTS_DIR, \"model_efficiency.json\"), 'w') as f:\n",
    "        json.dump(efficiency_export, f, indent=4)\n",
    "    print(\"\u2705 model_efficiency.json saved.\")\n",
    "\n",
    "# Tampilkan Tabel Ringkasan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- 3. Tabel Ringkasan Metrik untuk Edge Device ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "columns_to_show = [\"Model\", \"Total Parameter\", \"FLOPs\", \"Estimasi Ukuran 8-bit\", \"Estimasi Memori Aktivasi 8-bit\"]\n",
    "param_summary_df = pd.DataFrame(summary_list)[columns_to_show]\n",
    "\n",
    "def format_flops_str(f):\n",
    "    if f is None or f == 0: return \"N/A\"\n",
    "    return f'{f / 1e9:.2f} GFLOPs' if f > 1e9 else f'{f / 1e6:.2f} MFLOPs'\n",
    "def format_bytes_str(b):\n",
    "    if b is None or b == 0: return \"N/A\"\n",
    "    return f'{b / 1e6:.2f} MB' if b > 1e6 else f'{b / 1e3:.2f} KB'\n",
    "\n",
    "param_summary_df['Total Parameter'] = param_summary_df['Total Parameter'].map('{:,}'.format)\n",
    "param_summary_df['FLOPs'] = param_summary_df['FLOPs'].map(format_flops_str)\n",
    "param_summary_df['Estimasi Ukuran 8-bit'] = param_summary_df['Estimasi Ukuran 8-bit'].map(format_bytes_str)\n",
    "param_summary_df['Estimasi Memori Aktivasi 8-bit'] = param_summary_df['Estimasi Memori Aktivasi 8-bit'].map(format_bytes_str)\n",
    "\n",
    "print(param_summary_df.to_string(index=False))\n",
    "\n",
    "# Tampilkan Rincian Arsitektur\n",
    "print(\"\\n\\n\" + \"=\"*65)\n",
    "print(f\"--- 4. Rincian Arsitektur per Model ---\")\n",
    "print(\"=\"*65)\n",
    "for model_data in summary_list:\n",
    "    print(f\"\\n>>> {model_data['Model']}:\")\n",
    "    print(model_data['Architecture Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# 6. Loop Pelatihan (Sekarang Fokus Training Saja)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "datasets = {\n",
    "    'UASpeech': (uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    'TORGO': (torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "for dataset_name, (data_files, data_labels, data_speakers) in datasets.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"MEMPROSES TRAINING DATASET: {dataset_name}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    if len(data_files) == 0: continue\n",
    "\n",
    "    # Mapping Kelas & Split (Sama seperti sebelumnya)\n",
    "    unique_classes = sorted(list(set(data_labels)))\n",
    "    class_mapping = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to Numpy for easy indexing\n",
    "    X = np.array(data_files)\n",
    "    y = np.array(data_labels)\n",
    "    groups = np.array(data_speakers)\n",
    "    \n",
    "    # 1. SPLIT METODE PAPER 2 (STANDARD RANDOM SPLIT)\n",
    "    # Tujuan: Meniru metodologi Paper 2 untuk mendapatkan performa 97%.\n",
    "    # Menggunakan Stratified Shuffle Split, BUKAN Group Split.\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split 1: 80% Train, 20% (Test + Val)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split 2: 50% Test, 50% Val (Dari sisa 20% tadi) -> Jadi 10% Val, 10% Test totalnya\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Print Distribution\n",
    "    print(f\"--- Data Distribution ({dataset_name}) [Paper 2 Style - Random Split] ---\")\n",
    "    print(f\"[Train] Samples: {len(X_train)}\")\n",
    "    print(f\"[Val  ] Samples: {len(X_val)}\")\n",
    "    print(f\"[Test ] Samples: {len(X_test)}\")\n",
    "\n",
    "    for model_key, model_display_name in config.MODELS.items():\n",
    "        print(f\"\\n--- Training Pipeline: {model_display_name} @ {dataset_name} ---\")\n",
    "\n",
    "        # ... (Pipeline sama: Dataset -> Build -> Train -> Eval)\n",
    "        try:\n",
    "             # Tipe Fitur\n",
    "            feature_type = 'stft' if model_key == 'cnn_stft' else 'mfcc'\n",
    "            \n",
    "            # Create Dataset\n",
    "            train_ds = data_loader.create_tf_dataset(X_train, y_train, class_mapping, is_training=True, feature_type=feature_type)\n",
    "            val_ds = data_loader.create_tf_dataset(X_val, y_val, class_mapping, is_training=False, feature_type=feature_type)\n",
    "            test_ds = data_loader.create_tf_dataset(X_test, y_test, class_mapping, is_training=False, feature_type=feature_type)\n",
    "\n",
    "            # Get Input Shape from DS\n",
    "            input_shape = None\n",
    "            for feature, label in train_ds.take(1):\n",
    "                input_shape = feature.shape[1:]\n",
    "                break\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = models.get_model(model_key, input_shape, num_classes=len(unique_classes))\n",
    "\n",
    "            # Training\n",
    "            run_name = f\"{model_key}_{dataset_name}\"\n",
    "            history, time_taken = trainer.train_model(model, train_ds, val_ds, model_name=run_name)\n",
    "            print(f\"-> Training Done ({time_taken:.2f}s)\")\n",
    "            \n",
    "            # Eval & Benchmark Metrics\n",
    "            print(f\"-> Evaluating {run_name}...\")\n",
    "            import time\n",
    "            import json\n",
    "            from sklearn.metrics import classification_report\n",
    "            import numpy as np\n",
    "            \n",
    "            # 1. Inference Time Measurement\n",
    "            start_eval = time.time()\n",
    "            y_pred_probs = model.predict(test_ds)\n",
    "            end_eval = time.time()\n",
    "            \n",
    "            # Count samples via ds iteration\n",
    "            num_samples = 0\n",
    "            y_true = []\n",
    "            for features, labels in test_ds:\n",
    "                num_samples += features.shape[0]\n",
    "                y_true.extend(labels.numpy())\n",
    "                \n",
    "            inference_time_ms = ((end_eval - start_eval) / num_samples) * 1000\n",
    "            \n",
    "            # 2. Classification Report JSON\n",
    "            # Robust Logic for 1D (Sigmoid) vs 2D (Softmax) outputs\n",
    "            if y_pred_probs.ndim == 1 or y_pred_probs.shape[1] == 1:\n",
    "                # Binary/Sigmoid Case\n",
    "                y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "                prob_dysarthric = y_pred_probs.flatten()\n",
    "            else:\n",
    "                # Categorical/Softmax Case (Standard for this project)\n",
    "                y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "                prob_dysarthric = y_pred_probs[:, 1]\n",
    "            \n",
    "            report_dict = classification_report(y_true, y_pred, target_names=unique_classes, output_dict=True)\n",
    "            \n",
    "            # Save Report\n",
    "            report_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_report.json\")\n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(report_dict, f, indent=4)\n",
    "            print(f\"-> Report saved: {report_path}\")\n",
    "            \n",
    "            # 3. Append to Benchmark Summary\n",
    "            if 'benchmark_results' not in locals(): benchmark_results = []\n",
    "            \n",
    "            benchmark_entry = {\n",
    "                \"model\": model_key,\n",
    "                \"dataset\": dataset_name,\n",
    "                \"accuracy\": report_dict['accuracy'],\n",
    "                \"inference_time_ms\": inference_time_ms,\n",
    "                \"training_time_sec\": time_taken,\n",
    "                \"run_name\": run_name\n",
    "            }\n",
    "            benchmark_results.append(benchmark_entry)\n",
    "            \n",
    "            # 4. EXTENDED EVALUATION (Thesis Edition)\n",
    "            try:\n",
    "                from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "                import matplotlib.pyplot as plt\n",
    "                import seaborn as sns\n",
    "                import pandas as pd\n",
    "                \n",
    "                # A. Save Model Architecture\n",
    "                arch_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_arch.txt\")\n",
    "                with open(arch_path, 'w') as f:\n",
    "                    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "                print(f\"-> Architecture saved: {arch_path}\")\n",
    "\n",
    "                # B. Save Full Predictions (For Error Analysis)\n",
    "                # Re-map files for current dataset\n",
    "                curr_test_files = uaspeech_test_files if dataset_name == 'UASpeech' else torgo_test_files\n",
    "                \n",
    "                # Create DataFrame\n",
    "                # Ensure length matches (handle subsetting/filtering if diff)\n",
    "                if len(curr_test_files) == len(y_true):\n",
    "                    file_col = [os.path.basename(f) for f in curr_test_files]\n",
    "                else:\n",
    "                    # Fallback if split logic differs or shuffle happened invisibly\n",
    "                    file_col = ['Unknown_File'] * len(y_true)\n",
    "                \n",
    "                pred_df = pd.DataFrame({\n",
    "                    'file': file_col,\n",
    "                    'true_label': y_true,\n",
    "                    'pred_label': y_pred,\n",
    "                    'prob_dysarthric': prob_dysarthric,\n",
    "                    'is_correct': (np.array(y_true) == np.array(y_pred))\n",
    "                })\n",
    "                pred_csv_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_predictions.csv\")\n",
    "                pred_df.to_csv(pred_csv_path, index=False)\n",
    "                print(f\"-> Prediction Log saved: {pred_csv_path}\")\n",
    "\n",
    "                # C. Generate Static Plots (PNG for Thesis)\n",
    "                print(\"-> Generating Thesis Plots...\")\n",
    "                plot_learning_curve(history.history, model_display_name, run_name)\n",
    "                plot_confusion_matrix(y_true, y_pred, unique_classes, model_display_name, run_name)\n",
    "                plot_roc_curve(y_true, prob_dysarthric, model_display_name, run_name)\n",
    "                plot_pr_curve(y_true, prob_dysarthric, model_display_name, run_name)\n",
    "                plot_class_report(y_true, y_pred, unique_classes, model_display_name, run_name)\n",
    "                \n",
    "                # D. Dashboard JSON Export (Keep this for React UI)\n",
    "                cm = confusion_matrix(y_true, y_pred)\n",
    "                fpr, tpr, _ = roc_curve(y_true, prob_dysarthric)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                cm_list = cm.tolist()\n",
    "                indices = np.linspace(0, len(fpr)-1, 50).astype(int)\n",
    "                roc_data = [{\"x\": fpr[i], \"y\": tpr[i]} for i in indices]\n",
    "                \n",
    "                precision, recall, _ = precision_recall_curve(y_true, prob_dysarthric)\n",
    "                indices_pr = np.linspace(0, len(precision)-1, 50).astype(int)\n",
    "                pr_data = [{\"x\": recall[i], \"y\": precision[i]} for i in indices_pr]\n",
    "                \n",
    "                eval_export = {\n",
    "                    \"cm\": cm_list,\n",
    "                    \"roc\": roc_data,\n",
    "                    \"pr\": pr_data,\n",
    "                    \"auroc\": roc_auc\n",
    "                }\n",
    "                \n",
    "                eval_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_eval.json\")\n",
    "                with open(eval_path, 'w') as f:\n",
    "                    json.dump(eval_export, f)\n",
    "                print(f\"-> Extended Eval saved: {eval_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Failed to generate extended eval: {e}\")\n",
    "            \n",
    "            # Save Summary\n",
    "            summary_path = os.path.join(config.OUTPUTS_DIR, \"benchmark_summary.json\")\n",
    "            with open(summary_path, 'w') as f:\n",
    "                json.dump(benchmark_results, f, indent=4)\n",
    "            \n",
    "            # Standard Eval Print\n",
    "            trainer.evaluate_model(model, test_ds, unique_classes, model_name=run_name)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Training {model_display_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard_viz"
   },
   "outputs": [],
   "source": [
    "# 7. Visualisasi TensorBoard\n",
    "logs_base_dir = os.path.join(config.OUTPUTS_DIR, 'logs')\n",
    "%tensorboard --logdir \"{logs_base_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_comparison"
   },
   "outputs": [],
   "source": [
    "# 8. Final Benchmarking Plots (Paper 2 Comparison - CLONED)\\n",
    "import pandas as pd\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "import json\\n",
    "import glob\\n",
    "import os\\n",
    "import numpy as np\\n",
    "\\n",
    "# ------------------------------------------------------\\n",
    "# 1. LOAD ALL RESULTS\\n",
    "# ------------------------------------------------------\\n",
    "print(\\\"\\\\nLoading all evaluation results...\\\")\\n",
    "eval_files = glob.glob(os.path.join(config.OUTPUTS_DIR, '*_eval.json'))\\n",
    "all_results = {}\\n",
    "\\n",
    "if not eval_files:\\n",
    "    print(\\\"\u274c No evaluation files found. Skipping Comparison plots.\\\")\\n",
    "else:\\n",
    "    summary_path = os.path.join(config.OUTPUTS_DIR, \\\"benchmark_summary.json\\\")\\n",
    "    if os.path.exists(summary_path):\\n",
    "        with open(summary_path, 'r') as f: benchmark_data = json.load(f)\\n",
    "        # Convert to dictionary keyed by model name for easy lookup\\n",
    "        # Assuming structure based on 'run_name' or 'model'\\n",
    "        # Here we re-construct 'all_metrics' dictionary style from Paper 2\\n",
    "        all_metrics = {}\\n",
    "        training_times = {}\\n",
    "        \\n",
    "        for entry in benchmark_data:\\n",
    "            ds = entry.get('dataset', 'unknown')\\n",
    "            mod = entry.get('model', 'unknown')\\n",
    "            if ds not in all_metrics: all_metrics[ds] = {}\\n",
    "            if ds not in training_times: training_times[ds] = {}\\n",
    "            \\n",
    "            # Load full eval json for precision/recall/f1 breakdown if avail\\n",
    "            # But benchmark_summary usually has just accuracy. \\n",
    "            # Let's try to load the specific eval json for this run\\n",
    "            run_name = entry.get('run_name')\\n",
    "            eval_f = os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_eval.json\\\")\\n",
    "            \\n",
    "            # Default values\\n",
    "            prec, rec, f1 = 0, 0, 0\\n",
    "            \\n",
    "            # Extract from eval json output if needed, or rely on what we have. \\n",
    "            # Unlike Paper 2 which has 'all_results' in memory, here we reload from disk.\\n",
    "            # For simplicity, we trust the 'benchmark_summary' if we updated it to support F1/Prec/Recall\\n",
    "            # If not, we might need to rely on the 'report_bar' or similar.\\n",
    "            # Let's assume we want to plot mainly what is available.\\n",
    "            \\n",
    "            all_metrics[ds][mod] = {\\n",
    "                'accuracy': entry.get('accuracy', 0),\\n",
    "                # If these are missing in summary, we might default to 0\\n",
    "                'precision': entry.get('precision', 0),\\n",
    "                'recall': entry.get('recall', 0),\\n",
    "                'f1-score': entry.get('f1-score', 0)\\n",
    "            }\\n",
    "            training_times[ds][mod] = entry.get('training_time_sec', 0)\\n",
    "\\n",
    "    # ------------------------------------------------------\\n",
    "    # 2. TABEL KOMPARASI METRIK\\n",
    "    # ------------------------------------------------------\\n",
    "    print(f\\\"\\\\n{'='*90}\\\")\\n",
    "    print(\\\"\ud83d\udcca\ud83d\udcca\ud83d\udcca TABEL KOMPARASI METRIK ANTAR MODEL \ud83d\udcca\ud83d\udcca\ud83d\udcca\\\")\\n",
    "    print(f\\\"{'='*90}\\\")\\n",
    "    for dataset_name, metrics in all_metrics.items():\\n",
    "        print(f\\\"\\\\n### KOMPARASI UNTUK DATASET: {dataset_name.upper()} ###\\\")\\n",
    "        if metrics:\\n",
    "            df_comparison = pd.DataFrame(metrics).T\\n",
    "            df_comparison.index.name = 'Model'\\n",
    "            print(df_comparison.round(3).to_string())\\n",
    "        else:\\n",
    "            print(\\\"Tidak ada metrik untuk ditampilkan.\\\")\\n",
    "        print(\\\"-\\\" * 50)\\n",
    "\\n",
    "    # ------------------------------------------------------\\n",
    "    # 3. GRAFIK BATANG PERBANDINGAN METRIK\\n",
    "    # ------------------------------------------------------\\n",
    "    print(f\\\"\\\\n{'='*90}\\\")\\n",
    "    print(\\\"\ud83d\udcc8\ud83d\udcc8\ud83d\udcc8 GRAFIK BATANG PERBANDINGAN METRIK ANTAR MODEL \ud83d\udcc8\ud83d\udcc8\ud83d\udcc8\\\")\\n",
    "    print(f\\\"{'='*90}\\\")\\n",
    "    \\n",
    "    for dataset_name, metrics_data in all_metrics.items():\\n",
    "        if not metrics_data: continue\\n",
    "        try:\\n",
    "            models = list(metrics_data.keys())\\n",
    "            # Filter keys that actually exist in the data to avoid KeyErrors if summary is partial\\n",
    "            # We assume accuracy is always there.\\n",
    "            accuracies = [metrics_data[m].get('accuracy', 0) for m in models]\\n",
    "            # Precision/Recall might be missing if not saved in summary. In Paper 2 script they are calculated in-memory.\\n",
    "            # FOR NOW: We plot Accuracy primarily or use placeholders if 0.\\n",
    "            precisions = [metrics_data[m].get('precision', 0) for m in models]\\n",
    "            recalls = [metrics_data[m].get('recall', 0) for m in models]\\n",
    "            f1_scores = [metrics_data[m].get('f1-score', 0) for m in models]\\n",
    "\\n",
    "            index = np.arange(len(models)); bar_width = 0.2\\n",
    "            fig, ax = plt.subplots(figsize=(12, 7))\\n",
    "            colors = plt.cm.viridis(np.linspace(0.1, 0.9, 4))\\n",
    "\\n",
    "            bar1 = ax.bar(index - bar_width*1.5, accuracies, bar_width, label='Accuracy', color=colors[0])\\n",
    "            bar2 = ax.bar(index - bar_width/2, precisions, bar_width, label='Precision', color=colors[1])\\n",
    "            bar3 = ax.bar(index + bar_width/2, recalls, bar_width, label='Recall', color=colors[2])\\n",
    "            bar4 = ax.bar(index + bar_width*1.5, f1_scores, bar_width, label='F1-score', color=colors[3])\\n",
    "\\n",
    "            ax.set_xlabel('Model', fontweight='bold'); ax.set_ylabel('Skor', fontweight='bold')\\n",
    "            ax.set_title(f'Perbandingan Metrik Antar Model: {dataset_name.upper()}', fontsize=14, fontweight='bold')\\n",
    "            ax.set_xticks(index); ax.set_xticklabels(models, rotation=0)\\n",
    "            ax.legend(); ax.set_ylim(0, 1.1)\\n",
    "\\n",
    "            def autolabel(bars):\\n",
    "                for bar in bars:\\n",
    "                    height = bar.get_height()\\n",
    "                    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\\n",
    "                                xytext=(0, 3), textcoords=\\\"offset points\\\", ha='center', va='bottom', rotation=90)\\n",
    "\\n",
    "            autolabel(bar1); autolabel(bar2); autolabel(bar3); autolabel(bar4)\\n",
    "            fig.tight_layout()\\n",
    "            plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"comparison_metrics_{dataset_name}.png\\\"))\\n",
    "            plt.show()\\n",
    "        except Exception as e:\\n",
    "            print(f\\\"Error plot metric bar for {dataset_name}: {e}\\\")\\n",
    "\\n",
    "    # ------------------------------------------------------\\n",
    "    # 4. GRAFIK KURVA ROC GABUNGAN\\n",
    "    # ------------------------------------------------------\\n",
    "    print(f\\\"\\\\n{'='*90}\\\")\\n",
    "    print(\\\"\ud83d\udcc9\ud83d\udcc8\ud83d\udcc9 GRAFIK KURVA ROC (Combined) \ud83d\udcc9\ud83d\udcc8\ud83d\udcc9\\\")\\n",
    "    print(f\\\"{'='*90}\\\")\\n",
    "    \\n",
    "    # Group run_names by dataset from benchmark_results\\n",
    "    datasets_runs = {}\\n",
    "    for entry in benchmark_data:\\n",
    "         ds = entry.get('dataset', 'unknown')\\n",
    "         if ds not in datasets_runs: datasets_runs[ds] = []\\n",
    "         datasets_runs[ds].append(entry)\\n",
    "         \\n",
    "    for dataset_name, entries in datasets_runs.items():\\n",
    "        print(f\\\"\\\\n### KURVA ROC UNTUK DATASET: {dataset_name.upper()} ###\\\")\\n",
    "        plt.figure(figsize=(10, 8))\\n",
    "        has_plot = False\\n",
    "        for entry in entries:\\n",
    "            run_name = entry.get('run_name')\\n",
    "            model_name = entry.get('model')\\n",
    "            eval_path = os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_eval.json\\\")\\n",
    "            try:\\n",
    "                with open(eval_path, 'r') as f: data = json.load(f)\\n",
    "                roc_data = data.get('roc', [])\\n",
    "                if roc_data:\\n",
    "                    fpr = [p['x'] for p in roc_data]\\n",
    "                    tpr = [p['y'] for p in roc_data]\\n",
    "                    auroc = data.get('auroc', 0)\\n",
    "                    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {auroc:.2f})')\\n",
    "                    has_plot = True\\n",
    "            except: pass\\n",
    "            \\n",
    "        if has_plot:\\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Acak (AUC = 0.50)')\\n",
    "            plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\\n",
    "            plt.xlabel('False Positive Rate', fontsize=12); plt.ylabel('True Positive Rate', fontsize=12)\\n",
    "            plt.title(f'Kurva ROC Gabungan - {dataset_name.upper()}', fontsize=14, fontweight='bold')\\n",
    "            plt.legend(loc=\\\"lower right\\\", fontsize=10); plt.grid(True)\\n",
    "            plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"combined_roc_{dataset_name}.png\\\"))\\n",
    "            plt.show()\\n",
    "        else:\\n",
    "            # Don't show empty plot\\n",
    "            plt.close()\\n",
    "            \\n",
    "    # ------------------------------------------------------\\n",
    "    # 5. GRAFIK KURVA PRC GABUNGAN\\n",
    "    # ------------------------------------------------------\\n",
    "    print(f\\\"\\\\n{'='*90}\\\")\\n",
    "    print(\\\"\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf GRAFIK KURVA PRC (Combined) \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\\\")\\n",
    "    print(f\\\"{'='*90}\\\")\\n",
    "    \\n",
    "    for dataset_name, entries in datasets_runs.items():\\n",
    "        print(f\\\"\\\\n### KURVA PRC UNTUK DATASET: {dataset_name.upper()} ###\\\")\\n",
    "        plt.figure(figsize=(10, 8))\\n",
    "        has_plot = False\\n",
    "        \\n",
    "        for entry in entries:\\n",
    "            run_name = entry.get('run_name')\\n",
    "            model_name = entry.get('model')\\n",
    "            eval_path = os.path.join(config.OUTPUTS_DIR, f\\\"{run_name}_eval.json\\\")\\n",
    "            try:\\n",
    "                with open(eval_path, 'r') as f: data = json.load(f)\\n",
    "                pr_data = data.get('pr', [])\\n",
    "                if pr_data:\\n",
    "                    rec = [p['x'] for p in pr_data]\\n",
    "                    prec = [p['y'] for p in pr_data]\\n",
    "                    # Calculate AUPRC roughly or grab if saved. JSON usually doesn't have it explicitly unless added.\\n",
    "                    # Let's just label model name.\\n",
    "                    plt.plot(rec, prec, lw=2, label=f'{model_name}')\\n",
    "                    has_plot = True\\n",
    "            except: pass\\n",
    "            \\n",
    "        if has_plot:\\n",
    "            plt.xlabel('Recall', fontsize=12)\\n",
    "            plt.ylabel('Precision', fontsize=12)\\n",
    "            plt.title(f'Kurva PRC Gabungan - {dataset_name.upper()}', fontsize=14, fontweight='bold')\\n",
    "            plt.legend(loc=\\\"best\\\", fontsize=10)\\n",
    "            plt.grid(True)\\n",
    "            plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"combined_prc_{dataset_name}.png\\\"))\\n",
    "            plt.show()\\n",
    "        else:\\n",
    "            plt.close()\\n",
    "            \\n",
    "    # ------------------------------------------------------\\n",
    "    # 6. TRAINING TIME COMPARISON\\n",
    "    # ------------------------------------------------------\\n",
    "    print(f\\\"\\\\n{'='*90}\\\")\\n",
    "    print(\\\"\u23f1\ufe0f\u23f1\ufe0f\u23f1\ufe0f TABEL DAN GRAFIK WAKTU TRAINING \u23f1\ufe0f\u23f1\ufe0f\u23f1\ufe0f\\\")\\n",
    "    print(f\\\"{'='*90}\\\")\\n",
    "    \\n",
    "    for dataset_name, times in training_times.items():\\n",
    "         if not times: continue\\n",
    "         print(f\\\"\\\\n### WAKTU TRAINING UNTUK DATASET: {dataset_name.upper()} ###\\\")\\n",
    "         df_time = pd.DataFrame(list(times.items()), columns=['Model', 'Waktu (detik)'])\\n",
    "         print(df_time.to_string(index=False))\\n",
    "         \\n",
    "         plt.figure(figsize=(8, 5))\\n",
    "         plt.bar(df_time['Model'], df_time['Waktu (detik)'], color='#FF5722')\\n",
    "         plt.title(f'Waktu Training per Model: {dataset_name}', fontsize=14)\\n",
    "         plt.ylabel('Waktu (detik)'); plt.xlabel('Model')\\n",
    "         plt.xticks(rotation=45); plt.tight_layout()\\n",
    "         plt.savefig(os.path.join(config.OUTPUTS_DIR, f\\\"comparison_time_{dataset_name}.png\\\"))\\n",
    "         plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}