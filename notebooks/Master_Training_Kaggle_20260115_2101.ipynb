{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# Notebook Pelatihan Utama - Pengenalan Ucapan Disartria (KAGGLE VERSION)\n",
    "**Version:** 20260115_2101\\n",
    "\n",
    "**Tujuan:** Analisis Perbandingan **Lightweight CNN-STFT** (Diusulkan) vs **Model Transfer Learning**.\n",
    "**Platform:** Kaggle Kernels (GPU T4 x2).\n",
    "**Strategy:** Subject-Independent Split (Verified).\n",
    "\n",
    "## \ud83d\udccb Panduan Setup Kaggle\n",
    "1. **Add Data**: Upload folder `backend` anda sebagai Dataset (beri nama `thesis-backend` misalnya).\n",
    "2. **Add Data**: Cari dataset `UASpeech` dan `TORGO` (atau upload zip-nya jika punya privasi).\n",
    "3. **Internet**: Aktifkan Internet di menu Settings (kanan) jika perlu download via `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env_setup"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Environment & Path (Kaggle Symlink Fix)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "print(\"\ud83d\ude80 Memulai Setup Kaggle Environment...\")\n",
    "\n",
    "# A. Cari file 'config.py' dimanapun dia berada\n",
    "config_path = None\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    if 'config.py' in files:\n",
    "        config_path = os.path.join(root, 'config.py')\n",
    "        break\n",
    "\n",
    "if config_path:\n",
    "    print(f\"\u2705 Ditemukan Config di: {config_path}\")\n",
    "    source_dir = os.path.dirname(config_path)\n",
    "    \n",
    "    # B. Buat Symlink 'src' di Working Directory\n",
    "    # Tujuannya agar 'from src import config' SELALU jalan, tidak peduli struktur aslinya rusak/flatten\n",
    "    target_link = '/kaggle/working/src'\n",
    "    if os.path.exists(target_link):\n",
    "        if os.path.islink(target_link):\n",
    "            os.unlink(target_link)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(target_link)\n",
    "            \n",
    "    os.symlink(source_dir, target_link)\n",
    "    print(f\"\ud83d\udd17 Symlink dibuat: {target_link} -> {source_dir}\")\n",
    "    \n",
    "    # C. Tambah Working Dir ke Sys Path\n",
    "    if '/kaggle/working' not in sys.path:\n",
    "        sys.path.append('/kaggle/working')\n",
    "else:\n",
    "    print(\"\u274c FATAL: File 'config.py' tidak ditemukan di Input manapun!\")\n",
    "    print(\"   Pastikan Anda sudah 'Add Data' folder backend.\")\n",
    "\n",
    "# D. Setup Output Paths\n",
    "OUTPUT_ROOT = '/kaggle/working'\n",
    "LOCAL_DATA_ROOT = '/kaggle/working/data'\n",
    "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Environment Siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 2. Install Dependencies\n",
    "!pip install -q tensorflow-io\n",
    "!pip install -q pandas matplotlib seaborn scikit-learn librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# 3. Import Modul Proyek\n",
    "try:\n",
    "    from src import config, data_loader, models, trainer\n",
    "    print(\"\u2705 Modul berhasil diimport: config, data_loader, models, trainer\")\n",
    "\n",
    "    # Override Config untuk Kaggle Output\n",
    "    config.MODELS_DIR = os.path.join(OUTPUT_ROOT, 'models')\n",
    "    config.OUTPUTS_DIR = os.path.join(OUTPUT_ROOT, 'outputs')\n",
    "    os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(config.OUTPUTS_DIR, exist_ok=True)\n",
    "    print(f\"\ud83d\udcc2 Output Directory set to: {config.OUTPUTS_DIR}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Gagal import modul: {e}\")\n",
    "    print(\"Pastikan 'backend' terdeteksi dengan benar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_prep_func"
   },
   "outputs": [],
   "source": [
    "# 4. Persiapan Data (Kaggle Auto-Detect or Gdown)\n",
    "import shutil\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "# IDs Google Drive (Backup jika file tidak ada di Kaggle Dataset)\n",
    "UASPEECH_ID = '1L17F0SAkRk3rEjHDUyToLUvNp99sNMvE'\n",
    "TORGO_ID = '1YU7aCqa4qyn75XRdFPAWEqVv_1Qpl9cG'\n",
    "\n",
    "def setup_dataset_kaggle(name, file_id, extract_path):\n",
    "    print(f\"\\n--- Setup Dataset: {name} ---\")\n",
    "    \n",
    "    # 1. Cek di /kaggle/input (Siapa tau user sudah add data)\n",
    "    # Polanya: /kaggle/input/<name> atau /kaggle/input/<anything>/<name>\n",
    "    candidates = glob.glob(f'/kaggle/input/**/*{name}*', recursive=True)\n",
    "    \n",
    "    # Filter hanya folder yang valid (bukan file zip/meta)\n",
    "    potential_dirs = [c for c in candidates if os.path.isdir(c)]\n",
    "    \n",
    "    # Spesifik untuk TORGO/UASpeech foldernya biasanya 'UASpeech' atau 'TORGO'\n",
    "    for p in potential_dirs:\n",
    "        if os.path.basename(p).lower() == name.lower() or os.path.basename(p).lower() == f\"{name}_smalldataset\".lower():\n",
    "             print(f\"\u2705 Ditemukan Dataset di Input: {p}\")\n",
    "             return p\n",
    "\n",
    "    # 3. Jika tidak ketemu di Input, Coba Download (Gdown)\n",
    "    print(f\"\u26a0\ufe0f {name} tidak ditemukan di ke Kaggle Input. Mencoba download via Gdown...\")\n",
    "    \n",
    "    local_zip_path = os.path.join(extract_path, f\"{name}.zip\")\n",
    "    target_extract = os.path.join(extract_path, name)\n",
    "    \n",
    "    if os.path.exists(target_extract):\n",
    "         print(f\"\u2705 Dataset sudah ada di Working Dir: {target_extract}\")\n",
    "         return target_extract\n",
    "         \n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    gdown.download(url, local_zip_path, quiet=False)\n",
    "    \n",
    "    print(f\"Mengekstrak {name}...\")\n",
    "    subprocess.check_call(['unzip', '-o', '-q', local_zip_path, '-d', extract_path])\n",
    "    print(f\"\u2705 {name} Selesai diekstrak.\")\n",
    "    \n",
    "    # Handle nama folder TORGO yang kadang beda\n",
    "    if name == 'TORGO' and not os.path.exists(target_extract):\n",
    "         alt = os.path.join(extract_path, 'TORGO_smalldataset')\n",
    "         if os.path.exists(alt): return alt\n",
    "         \n",
    "    return target_extract\n",
    "\n",
    "# Jalankan Setup\n",
    "uaspeech_path = setup_dataset_kaggle('UASpeech', UASPEECH_ID, LOCAL_DATA_ROOT)\n",
    "torgo_path = setup_dataset_kaggle('TORGO', TORGO_ID, LOCAL_DATA_ROOT)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOADING DATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMemuat Path File...\")\n",
    "\n",
    "# Load Path File Audio\n",
    "uaspeech_files, uaspeech_labels, uaspeech_speakers = data_loader.get_file_paths(uaspeech_path, 'UASpeech')\n",
    "torgo_files, torgo_labels, torgo_speakers = data_loader.get_file_paths(torgo_path, 'TORGO')\n",
    "\n",
    "# --- GENERATE DATASET STATS FOR DASHBOARD ---\n",
    "import json\n",
    "print(\"Generating Dataset Statistics...\")\n",
    "\n",
    "def get_stats(name, files, labels, speakers):\n",
    "    unique_lbl = list(set(labels))\n",
    "    counts = {l: 0 for l in unique_lbl}\n",
    "    for l in labels: counts[l] += 1\n",
    "    \n",
    "    summary = []\n",
    "    for l in unique_lbl:\n",
    "        cat = \"Dysarthric\" if l == 1 else \"Control\"\n",
    "        total = counts[l]\n",
    "        summary.append({\n",
    "            \"category\": cat,\n",
    "            \"speakers\": len(set(speakers)), # Rough approx\n",
    "            \"totalRaw\": total,\n",
    "            \"trainRaw\": int(total * 0.8),\n",
    "            \"testRaw\": total - int(total * 0.8)\n",
    "        })\n",
    "        \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"stats\": {\n",
    "            \"samples\": f\"{len(files):,}\",\n",
    "            \"classes\": str(len(unique_lbl)),\n",
    "            \"avgLen\": \"N/A\" # Skip expensive calc\n",
    "        },\n",
    "        \"summaryData\": summary\n",
    "    }\n",
    "\n",
    "stats_export = {\n",
    "    \"uaspeech\": get_stats('UASpeech', uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    \"torgo\": get_stats('TORGO', torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "with open(os.path.join(config.OUTPUTS_DIR, \"dataset_stats.json\"), 'w') as f:\n",
    "    json.dump(stats_export, f, indent=4)\n",
    "print(\"\u2705 dataset_stats.json saved.\")\n",
    "\n",
    "# --- GENERATE REAL EDA SAMPLES (Audio + Signals) ---\n",
    "print(\"Generating EDA Samples (Waveform & Spectrogram data)...\")\n",
    "import random\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "samples_out_dir = os.path.join(config.OUTPUTS_DIR, \"samples\")\n",
    "os.makedirs(samples_out_dir, exist_ok=True)\n",
    "\n",
    "eda_export = {}\n",
    "\n",
    "# Iterate over both datasets\n",
    "for ds_name, (ds_files, ds_labels, ds_speakers) in [('uaspeech', (uaspeech_files, uaspeech_labels, uaspeech_speakers)), ('torgo', (torgo_files, torgo_labels, torgo_speakers))]:\n",
    "    eda_export[ds_name] = {'dysarthric': [], 'control': []}\n",
    "    \n",
    "    # Binary Classification Logic (Assuming 1=Dysarthric)\n",
    "    # Note: If labels are different, adjust accordingly.\n",
    "    # Based on previous cell: \"Dysarthric\" if l == 1 else \"Control\"\n",
    "    \n",
    "    indices_dys = [i for i, x in enumerate(ds_labels) if x == 1]\n",
    "    indices_ctrl = [i for i, x in enumerate(ds_labels) if x != 1]\n",
    "    \n",
    "    # Pick 5 random from each\n",
    "    picks_dys = random.sample(indices_dys, min(5, len(indices_dys)))\n",
    "    picks_ctrl = random.sample(indices_ctrl, min(5, len(indices_ctrl)))\n",
    "    \n",
    "    for category, picks in [('dysarthric', picks_dys), ('control', picks_ctrl)]:\n",
    "        for idx in picks:\n",
    "            src = ds_files[idx]\n",
    "            fname = f\"{ds_name}_{os.path.basename(src)}\" # Prefix to avoid collision\n",
    "            dst = os.path.join(samples_out_dir, fname)\n",
    "            shutil.copy(src, dst)\n",
    "            \n",
    "            # Analyze Signal\n",
    "            try:\n",
    "                y, sr = librosa.load(src, sr=16000)\n",
    "                duration = len(y) / sr\n",
    "                \n",
    "                # 1. Waveform (100 points max, absolute amplitude)\n",
    "                hop_len = max(1, len(y) // 80) # 80 bars\n",
    "                waveform = [float(np.max(np.abs(y[i:i+hop_len]))) for i in range(0, len(y), hop_len)][:80]\n",
    "                # Normalize waveform 0-100 for CSS height\n",
    "                max_val = max(waveform) if waveform else 1\n",
    "                waveform = [int((v / max_val) * 100) for v in waveform]\n",
    "                \n",
    "                # 2. Mel Spectrogram (Low Res for JSON: 40 bands x 60 time steps)\n",
    "                # Enough for visual \"texture\" without bloating JSON\n",
    "                n_mels = 40\n",
    "                hop_spec = len(y) // 60\n",
    "                if hop_spec < 512: hop_spec = 512 # Minimum hop\n",
    "                \n",
    "                S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_spec)\n",
    "                S_db = librosa.power_to_db(S, ref=np.max)\n",
    "                \n",
    "                # Normalize 0-1\n",
    "                min_db, max_db = S_db.min(), S_db.max()\n",
    "                S_norm = (S_db - min_db) / (max_db - min_db)\n",
    "                \n",
    "                # Ensure dimensions (cut if too long)\n",
    "                if S_norm.shape[1] > 60: S_norm = S_norm[:, :60]\n",
    "                \n",
    "                spectrogram = S_norm.tolist() # List of lists\n",
    "                \n",
    "                eda_export[ds_name][category].append({\n",
    "                    \"id\": os.path.splitext(fname)[0],\n",
    "                    \"name\": fname,\n",
    "                    \"duration\": f\"{duration:.1f}s\",\n",
    "                    \"durationSec\": duration,\n",
    "                    \"type\": category,\n",
    "                    \"severity\": \"Unknown\",\n",
    "                    \"waveform\": waveform,\n",
    "                    \"spectrogram\": spectrogram,\n",
    "                    \"url\": f\"/static/samples/{fname}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Error processing {fname}: {e}\")\n",
    "\n",
    "with open(os.path.join(config.OUTPUTS_DIR, \"eda_samples.json\"), 'w') as f:\n",
    "    json.dump(eda_export, f)\n",
    "print(\"\u2705 eda_samples.json saved (Audio & Visuals).\")\n",
    "\n",
    "print(\"Data terload. Siap training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_analysis"
   },
   "outputs": [],
   "source": [
    "# 5. ANALISIS MODEL & PERBANDINGAN STRUKTUR (WAJIB PAPER 2)\n",
    "# Bagian ini dipisahkan agar analisa FLOPs, Parameter, dan Memory terlihat jelas sebelum Training dimulai.\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 2. Membangun dan Meringkas Semua Arsitektur Model ---\")\n",
    "summary_list = []\n",
    "\n",
    "# Setup Input Shape Standar untuk Analisa (3 Channel untuk Model TL, 1 Channel untuk STFT)\n",
    "# Fix: Gunakan MFCC_MAX_LEN yang benar dari config\n",
    "# UPDATED: ImageNet models need 3 channels (RGB emulation)\n",
    "input_shape_mfcc = (config.N_MFCC, config.MFCC_MAX_LEN, 3)\n",
    "# Fix: Hitung N_STFT dari N_FFT/2 + 1 (Spectrogram Height)\n",
    "n_stft_bins = (config.N_FFT // 2) + 1\n",
    "input_shape_stft = (n_stft_bins, config.MFCC_MAX_LEN, 1)\n",
    "\n",
    "for model_key, model_display_name in config.MODELS.items():\n",
    "    print(f\"Menganalisis arsitektur untuk: {model_display_name}...\")\n",
    "    \n",
    "    # Tentukan input shape berdasarkan jenis model\n",
    "    current_input_shape = input_shape_stft if model_key == 'cnn_stft' else input_shape_mfcc\n",
    "    \n",
    "    # Build Model\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        model = models.get_model(model_key, current_input_shape, num_classes=2)\n",
    "        \n",
    "        # Hitung Metrik\n",
    "        total_params = model.count_params()\n",
    "        # Hitung FLOPs\n",
    "        flops = trainer.get_flops(model)\n",
    "        peak_mem_32bit, disk_size_32bit = trainer.get_model_memory_usage(model)\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Gagal build/metric {model_display_name}: {e}\")\n",
    "        flops = 0; peak_mem_32bit = 0; disk_size_32bit = 0\n",
    "        # Dummy summary\n",
    "        architecture_summary = \"Error building model\"\n",
    "    else:\n",
    "        # Capture Summary\n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        architecture_summary = stream.getvalue()\n",
    "        stream.close()\n",
    "\n",
    "    summary_list.append({\n",
    "        \"Model\": model_display_name,\n",
    "        \"Total Parameter\": total_params,\n",
    "        \"FLOPs\": flops,\n",
    "        \"Ukuran di Disk (32-bit)\": disk_size_32bit,\n",
    "        \"Estimasi Ukuran 8-bit\": disk_size_32bit / 4,\n",
    "        \"Estimasi Memori Aktivasi 8-bit\": peak_mem_32bit / 4,\n",
    "        \"Architecture Summary\": architecture_summary\n",
    "    })\n",
    "\n",
    "    # --- SAVE EFFICIENCY METRICS (JSON) ---\n",
    "    efficiency_export = {}\n",
    "    for item in summary_list:\n",
    "        # Clean up keys for JSON export\n",
    "        efficiency_export[item['Model']] = {\n",
    "            \"params\": str(item['Total Parameter']),\n",
    "            \"flops\": str(item['FLOPs']),\n",
    "            \"size\": f\"{item['Estimasi Ukuran 8-bit'] / 1024:.2f} MB\",\n",
    "            \"activation\": f\"{item['Estimasi Memori Aktivasi 8-bit'] / 1024:.2f} KB\"\n",
    "        }\n",
    "    \n",
    "    with open(os.path.join(config.OUTPUTS_DIR, \"model_efficiency.json\"), 'w') as f:\n",
    "        json.dump(efficiency_export, f, indent=4)\n",
    "    print(\"\u2705 model_efficiency.json saved.\")\n",
    "\n",
    "# Tampilkan Tabel Ringkasan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- 3. Tabel Ringkasan Metrik untuk Edge Device ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "columns_to_show = [\"Model\", \"Total Parameter\", \"FLOPs\", \"Estimasi Ukuran 8-bit\", \"Estimasi Memori Aktivasi 8-bit\"]\n",
    "param_summary_df = pd.DataFrame(summary_list)[columns_to_show]\n",
    "\n",
    "def format_flops_str(f):\n",
    "    if f is None or f == 0: return \"N/A\"\n",
    "    return f'{f / 1e9:.2f} GFLOPs' if f > 1e9 else f'{f / 1e6:.2f} MFLOPs'\n",
    "def format_bytes_str(b):\n",
    "    if b is None or b == 0: return \"N/A\"\n",
    "    return f'{b / 1e6:.2f} MB' if b > 1e6 else f'{b / 1e3:.2f} KB'\n",
    "\n",
    "param_summary_df['Total Parameter'] = param_summary_df['Total Parameter'].map('{:,}'.format)\n",
    "param_summary_df['FLOPs'] = param_summary_df['FLOPs'].map(format_flops_str)\n",
    "param_summary_df['Estimasi Ukuran 8-bit'] = param_summary_df['Estimasi Ukuran 8-bit'].map(format_bytes_str)\n",
    "param_summary_df['Estimasi Memori Aktivasi 8-bit'] = param_summary_df['Estimasi Memori Aktivasi 8-bit'].map(format_bytes_str)\n",
    "\n",
    "print(param_summary_df.to_string(index=False))\n",
    "\n",
    "# Tampilkan Rincian Arsitektur\n",
    "print(\"\\n\\n\" + \"=\"*65)\n",
    "print(f\"--- 4. Rincian Arsitektur per Model ---\")\n",
    "print(\"=\"*65)\n",
    "for model_data in summary_list:\n",
    "    print(f\"\\n>>> {model_data['Model']}:\")\n",
    "    print(model_data['Architecture Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# 6. Loop Pelatihan (Sekarang Fokus Training Saja)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "datasets = {\n",
    "    'UASpeech': (uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    'TORGO': (torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "for dataset_name, (data_files, data_labels, data_speakers) in datasets.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"MEMPROSES TRAINING DATASET: {dataset_name}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    if len(data_files) == 0: continue\n",
    "\n",
    "    # Mapping Kelas & Split (Sama seperti sebelumnya)\n",
    "    unique_classes = sorted(list(set(data_labels)))\n",
    "    class_mapping = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to Numpy for easy indexing\n",
    "    X = np.array(data_files)\n",
    "    y = np.array(data_labels)\n",
    "    groups = np.array(data_speakers)\n",
    "    \n",
    "    # 1. SPLIT METODE PAPER 2 (STANDARD RANDOM SPLIT)\n",
    "    # Tujuan: Meniru metodologi Paper 2 untuk mendapatkan performa 97%.\n",
    "    # Menggunakan Stratified Shuffle Split, BUKAN Group Split.\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split 1: 80% Train, 20% (Test + Val)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split 2: 50% Test, 50% Val (Dari sisa 20% tadi) -> Jadi 10% Val, 10% Test totalnya\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Print Distribution\n",
    "    print(f\"--- Data Distribution ({dataset_name}) [Paper 2 Style - Random Split] ---\")\n",
    "    print(f\"[Train] Samples: {len(X_train)}\")\n",
    "    print(f\"[Val  ] Samples: {len(X_val)}\")\n",
    "    print(f\"[Test ] Samples: {len(X_test)}\")\n",
    "\n",
    "    for model_key, model_display_name in config.MODELS.items():\n",
    "        print(f\"\\n--- Training Pipeline: {model_display_name} @ {dataset_name} ---\")\n",
    "\n",
    "        # ... (Pipeline sama: Dataset -> Build -> Train -> Eval)\n",
    "        try:\n",
    "             # Tipe Fitur\n",
    "            feature_type = 'stft' if model_key == 'cnn_stft' else 'mfcc'\n",
    "            \n",
    "            # Create Dataset\n",
    "            train_ds = data_loader.create_tf_dataset(X_train, y_train, class_mapping, is_training=True, feature_type=feature_type)\n",
    "            val_ds = data_loader.create_tf_dataset(X_val, y_val, class_mapping, is_training=False, feature_type=feature_type)\n",
    "            test_ds = data_loader.create_tf_dataset(X_test, y_test, class_mapping, is_training=False, feature_type=feature_type)\n",
    "\n",
    "            # Get Input Shape from DS\n",
    "            input_shape = None\n",
    "            for feature, label in train_ds.take(1):\n",
    "                input_shape = feature.shape[1:]\n",
    "                break\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = models.get_model(model_key, input_shape, num_classes=len(unique_classes))\n",
    "\n",
    "            # Training\n",
    "            run_name = f\"{model_key}_{dataset_name}\"\n",
    "            history, time_taken = trainer.train_model(model, train_ds, val_ds, model_name=run_name)\n",
    "            print(f\"-> Training Done ({time_taken:.2f}s)\")\n",
    "            \n",
    "            # Eval & Benchmark Metrics\n",
    "            print(f\"-> Evaluating {run_name}...\")\n",
    "            import time\n",
    "            import json\n",
    "            from sklearn.metrics import classification_report\n",
    "            import numpy as np\n",
    "            \n",
    "            # 1. Inference Time Measurement\n",
    "            start_eval = time.time()\n",
    "            y_pred_probs = model.predict(test_ds)\n",
    "            end_eval = time.time()\n",
    "            \n",
    "            # Count samples via ds iteration\n",
    "            num_samples = 0\n",
    "            y_true = []\n",
    "            for features, labels in test_ds:\n",
    "                num_samples += features.shape[0]\n",
    "                y_true.extend(labels.numpy())\n",
    "                \n",
    "            inference_time_ms = ((end_eval - start_eval) / num_samples) * 1000\n",
    "            \n",
    "            # 2. Classification Report JSON\n",
    "            y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "            report_dict = classification_report(y_true, y_pred, target_names=unique_classes, output_dict=True)\n",
    "            \n",
    "            # Save Report\n",
    "            report_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_report.json\")\n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(report_dict, f, indent=4)\n",
    "            print(f\"-> Report saved: {report_path}\")\n",
    "            \n",
    "            # 3. Append to Benchmark Summary\n",
    "            if 'benchmark_results' not in locals(): benchmark_results = []\n",
    "            \n",
    "            benchmark_entry = {\n",
    "                \"model\": model_key,\n",
    "                \"dataset\": dataset_name,\n",
    "                \"accuracy\": report_dict['accuracy'],\n",
    "                \"inference_time_ms\": inference_time_ms,\n",
    "                \"training_time_sec\": time_taken,\n",
    "                \"run_name\": run_name\n",
    "            }\n",
    "            benchmark_results.append(benchmark_entry)\n",
    "            \n",
    "            # 4. EXTENDED EVALUATION (Thesis Edition)\n",
    "            try:\n",
    "                from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "                import matplotlib.pyplot as plt\n",
    "                import seaborn as sns\n",
    "                import pandas as pd\n",
    "                \n",
    "                # A. Save Model Architecture\n",
    "                arch_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_arch.txt\")\n",
    "                with open(arch_path, 'w') as f:\n",
    "                    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "                print(f\"-> Architecture saved: {arch_path}\")\n",
    "\n",
    "                # B. Save Full Predictions (For Error Analysis)\n",
    "                # Re-map files for current dataset\n",
    "                curr_test_files = uaspeech_test_files if dataset_name == 'UASpeech' else torgo_test_files\n",
    "                \n",
    "                # Create DataFrame\n",
    "                pred_df = pd.DataFrame({\n",
    "                    'file': [os.path.basename(f) for f in curr_test_files] if len(curr_test_files) == len(y_true) else ['Unknown']*len(y_true),\n",
    "                    'true_label': y_true,\n",
    "                    'pred_label': y_pred,\n",
    "                    'prob_dysarthric': y_pred_probs[:, 1],\n",
    "                    'is_correct': (np.array(y_true) == np.array(y_pred))\n",
    "                })\n",
    "                pred_csv_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_predictions.csv\")\n",
    "                pred_df.to_csv(pred_csv_path, index=False)\n",
    "                print(f\"-> Prediction Log saved: {pred_csv_path}\")\n",
    "\n",
    "                # C. Generate & Save Static Plots (PNG for Thesis)\n",
    "                # 1. Confusion Matrix\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                cm = confusion_matrix(y_true, y_pred)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "                plt.title(f'Confusion Matrix - {model_display_name}')\n",
    "                plt.ylabel('True Label')\n",
    "                plt.xlabel('Predicted Label')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(config.OUTPUTS_DIR, f\"{run_name}_cm.png\"))\n",
    "                plt.close()\n",
    "\n",
    "                # 2. ROC Curve\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_pred_probs[:, 1])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC - {model_display_name}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(os.path.join(config.OUTPUTS_DIR, f\"{run_name}_roc.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "                # JSON Export for Dashboard\n",
    "                cm_list = cm.tolist()\n",
    "                indices = np.linspace(0, len(fpr)-1, 50).astype(int)\n",
    "                roc_data = [{\"x\": fpr[i], \"y\": tpr[i]} for i in indices]\n",
    "                \n",
    "                precision, recall, _ = precision_recall_curve(y_true, y_pred_probs[:, 1])\n",
    "                indices_pr = np.linspace(0, len(precision)-1, 50).astype(int)\n",
    "                pr_data = [{\"x\": recall[i], \"y\": precision[i]} for i in indices_pr]\n",
    "                \n",
    "                eval_export = {\n",
    "                    \"cm\": cm_list,\n",
    "                    \"roc\": roc_data,\n",
    "                    \"pr\": pr_data,\n",
    "                    \"auroc\": roc_auc\n",
    "                }\n",
    "                \n",
    "                eval_path = os.path.join(config.OUTPUTS_DIR, f\"{run_name}_eval.json\")\n",
    "                with open(eval_path, 'w') as f:\n",
    "                    json.dump(eval_export, f)\n",
    "                print(f\"-> Extended Eval saved: {eval_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Failed to generate extended eval: {e}\")\n",
    "            \n",
    "            # Save Summary\n",
    "            summary_path = os.path.join(config.OUTPUTS_DIR, \"benchmark_summary.json\")\n",
    "            with open(summary_path, 'w') as f:\n",
    "                json.dump(benchmark_results, f, indent=4)\n",
    "            \n",
    "            # Standard Eval Print\n",
    "            trainer.evaluate_model(model, test_ds, unique_classes, model_name=run_name)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Training {model_display_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard_viz"
   },
   "outputs": [],
   "source": [
    "# 7. Visualisasi TensorBoard\n",
    "logs_base_dir = os.path.join(config.OUTPUTS_DIR, 'logs')\n",
    "%tensorboard --logdir \"{logs_base_dir}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}