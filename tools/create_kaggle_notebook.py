import nbformat as nbf
import json
import os
import sys

# Output File
target_file = '../notebooks/Master_Training_Kaggle.ipynb'
os.makedirs(os.path.dirname(target_file), exist_ok=True)

# Notebook Content Structure
notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# Notebook Pelatihan Utama - Pengenalan Ucapan Disartria (KAGGLE VERSION)\n",
    "\n",
    "**Tujuan:** Analisis Perbandingan **Lightweight CNN-STFT** (Diusulkan) vs **Model Transfer Learning**.\n",
    "**Platform:** Kaggle Kernels (GPU T4 x2).\n",
    "**Strategy:** Subject-Independent Split (Verified).\n",
    "\n",
    "## üìã Panduan Setup Kaggle\n",
    "1. **Add Data**: Upload folder `backend` anda sebagai Dataset (beri nama `thesis-backend` misalnya).\n",
    "2. **Add Data**: Cari dataset `UASpeech` dan `TORGO` (atau upload zip-nya jika punya privasi).\n",
    "3. **Internet**: Aktifkan Internet di menu Settings (kanan) jika perlu download via `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "env_setup"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Environment & Path (Kaggle Symlink Fix)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "print(\"üöÄ Memulai Setup Kaggle Environment...\")\n",
    "\n",
    "# A. Cari file 'config.py' dimanapun dia berada\n",
    "config_path = None\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    if 'config.py' in files:\n",
    "        config_path = os.path.join(root, 'config.py')\n",
    "        break\n",
    "\n",
    "if config_path:\n",
    "    print(f\"‚úÖ Ditemukan Config di: {config_path}\")\n",
    "    source_dir = os.path.dirname(config_path)\n",
    "    \n",
    "    # B. Buat Symlink 'src' di Working Directory\n",
    "    # Tujuannya agar 'from src import config' SELALU jalan, tidak peduli struktur aslinya rusak/flatten\n",
    "    target_link = '/kaggle/working/src'\n",
    "    if os.path.exists(target_link):\n",
    "        if os.path.islink(target_link):\n",
    "            os.unlink(target_link)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(target_link)\n",
    "            \n",
    "    os.symlink(source_dir, target_link)\n",
    "    print(f\"üîó Symlink dibuat: {target_link} -> {source_dir}\")\n",
    "    \n",
    "    # C. Tambah Working Dir ke Sys Path\n",
    "    if '/kaggle/working' not in sys.path:\n",
    "        sys.path.append('/kaggle/working')\n",
    "else:\n",
    "    print(\"‚ùå FATAL: File 'config.py' tidak ditemukan di Input manapun!\")\n",
    "    print(\"   Pastikan Anda sudah 'Add Data' folder backend.\")\n",
    "\n",
    "# D. Setup Output Paths\n",
    "OUTPUT_ROOT = '/kaggle/working'\n",
    "LOCAL_DATA_ROOT = '/kaggle/working/data'\n",
    "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Environment Siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 2. Install Dependencies\n",
    "!pip install -q tensorflow-io\n",
    "!pip install -q pandas matplotlib seaborn scikit-learn librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# 3. Import Modul Proyek\n",
    "try:\n",
    "    from src import config, data_loader, models, trainer\n",
    "    print(\"‚úÖ Modul berhasil diimport: config, data_loader, models, trainer\")\n",
    "\n",
    "    # Override Config untuk Kaggle Output\n",
    "    config.MODELS_DIR = os.path.join(OUTPUT_ROOT, 'models')\n",
    "    config.OUTPUTS_DIR = os.path.join(OUTPUT_ROOT, 'outputs')\n",
    "    os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(config.OUTPUTS_DIR, exist_ok=True)\n",
    "    print(f\"üìÇ Output Directory set to: {config.OUTPUTS_DIR}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Gagal import modul: {e}\")\n",
    "    print(\"Pastikan 'backend' terdeteksi dengan benar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "data_prep_func"
   },
   "outputs": [],
   "source": [
    "# 4. Persiapan Data (Kaggle Auto-Detect or Gdown)\n",
    "import shutil\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "# IDs Google Drive (Backup jika file tidak ada di Kaggle Dataset)\n",
    "UASPEECH_ID = '1L17F0SAkRk3rEjHDUyToLUvNp99sNMvE'\n",
    "TORGO_ID = '1YU7aCqa4qyn75XRdFPAWEqVv_1Qpl9cG'\n",
    "\n",
    "def setup_dataset_kaggle(name, file_id, extract_path):\n",
    "    print(f\"\\n--- Setup Dataset: {name} ---\")\n",
    "    \n",
    "    # 1. Cek di /kaggle/input (Siapa tau user sudah add data)\n",
    "    # Polanya: /kaggle/input/<name> atau /kaggle/input/<anything>/<name>\n",
    "    candidates = glob.glob(f'/kaggle/input/**/*{name}*', recursive=True)\n",
    "    \n",
    "    # Filter hanya folder yang valid (bukan file zip/meta)\n",
    "    potential_dirs = [c for c in candidates if os.path.isdir(c)]\n",
    "    \n",
    "    # Spesifik untuk TORGO/UASpeech foldernya biasanya 'UASpeech' atau 'TORGO'\n",
    "    for p in potential_dirs:\n",
    "        if os.path.basename(p).lower() == name.lower() or os.path.basename(p).lower() == f\"{name}_smalldataset\".lower():\n",
    "             print(f\"‚úÖ Ditemukan Dataset di Input: {p}\")\n",
    "             return p\n",
    "\n",
    "    # 3. Jika tidak ketemu di Input, Coba Download (Gdown)\n",
    "    print(f\"‚ö†Ô∏è {name} tidak ditemukan di ke Kaggle Input. Mencoba download via Gdown...\")\n",
    "    \n",
    "    local_zip_path = os.path.join(extract_path, f\"{name}.zip\")\n",
    "    target_extract = os.path.join(extract_path, name)\n",
    "    \n",
    "    if os.path.exists(target_extract):\n",
    "         print(f\"‚úÖ Dataset sudah ada di Working Dir: {target_extract}\")\n",
    "         return target_extract\n",
    "         \n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    gdown.download(url, local_zip_path, quiet=False)\n",
    "    \n",
    "    print(f\"Mengekstrak {name}...\")\n",
    "    subprocess.check_call(['unzip', '-o', '-q', local_zip_path, '-d', extract_path])\n",
    "    print(f\"‚úÖ {name} Selesai diekstrak.\")\n",
    "    \n",
    "    # Handle nama folder TORGO yang kadang beda\n",
    "    if name == 'TORGO' and not os.path.exists(target_extract):\n",
    "         alt = os.path.join(extract_path, 'TORGO_smalldataset')\n",
    "         if os.path.exists(alt): return alt\n",
    "         \n",
    "    return target_extract\n",
    "\n",
    "# Jalankan Setup\n",
    "uaspeech_path = setup_dataset_kaggle('UASpeech', UASPEECH_ID, LOCAL_DATA_ROOT)\n",
    "torgo_path = setup_dataset_kaggle('TORGO', TORGO_ID, LOCAL_DATA_ROOT)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOADING DATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMemuat Path File...\")\n",
    "\n",
    "# Load Path File Audio\n",
    "uaspeech_files, uaspeech_labels, uaspeech_speakers = data_loader.get_file_paths(uaspeech_path, 'UASpeech')\n",
    "torgo_files, torgo_labels, torgo_speakers = data_loader.get_file_paths(torgo_path, 'TORGO')\n",
    "\n",
    "print(\"Data terload. Siap training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "model_analysis"
   },
   "outputs": [],
   "source": [
    "# 5. ANALISIS MODEL & PERBANDINGAN STRUKTUR (WAJIB PAPER 2)\n",
    "# Bagian ini dipisahkan agar analisa FLOPs, Parameter, dan Memory terlihat jelas sebelum Training dimulai.\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 2. Membangun dan Meringkas Semua Arsitektur Model ---\")\n",
    "summary_list = []\n",
    "\n",
    "# Setup Input Shape Standar untuk Analisa (4 Channel untuk Model TL, 1 Channel untuk STFT)\n",
    "# Fix: Gunakan MFCC_MAX_LEN yang benar dari config\n",
    "input_shape_mfcc = (config.N_MFCC, config.MFCC_MAX_LEN, 1)\n",
    "# Fix: Hitung N_STFT dari N_FFT/2 + 1 (Spectrogram Height)\n",
    "n_stft_bins = (config.N_FFT // 2) + 1\n",
    "input_shape_stft = (n_stft_bins, config.MFCC_MAX_LEN, 1)\n",
    "\n",
    "for model_key, model_display_name in config.MODELS.items():\n",
    "    print(f\"Menganalisis arsitektur untuk: {model_display_name}...\")\n",
    "    \n",
    "    # Tentukan input shape berdasarkan jenis model\n",
    "    current_input_shape = input_shape_stft if model_key == 'cnn_stft' else input_shape_mfcc\n",
    "    \n",
    "    # Build Model\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        model = models.get_model(model_key, current_input_shape, num_classes=2)\n",
    "        \n",
    "        # Hitung Metrik\n",
    "        total_params = model.count_params()\n",
    "        # Hitung FLOPs\n",
    "        flops = trainer.get_flops(model)\n",
    "        peak_mem_32bit, disk_size_32bit = trainer.get_model_memory_usage(model)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Gagal build/metric {model_display_name}: {e}\")\n",
    "        flops = 0; peak_mem_32bit = 0; disk_size_32bit = 0\n",
    "        # Dummy summary\n",
    "        architecture_summary = \"Error building model\"\n",
    "    else:\n",
    "        # Capture Summary\n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        architecture_summary = stream.getvalue()\n",
    "        stream.close()\n",
    "\n",
    "    summary_list.append({\n",
    "        \"Model\": model_display_name,\n",
    "        \"Total Parameter\": total_params,\n",
    "        \"FLOPs\": flops,\n",
    "        \"Ukuran di Disk (32-bit)\": disk_size_32bit,\n",
    "        \"Estimasi Ukuran 8-bit\": disk_size_32bit / 4,\n",
    "        \"Estimasi Memori Aktivasi 8-bit\": peak_mem_32bit / 4,\n",
    "        \"Architecture Summary\": architecture_summary\n",
    "    })\n",
    "\n",
    "# Tampilkan Tabel Ringkasan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- 3. Tabel Ringkasan Metrik untuk Edge Device ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "columns_to_show = [\"Model\", \"Total Parameter\", \"FLOPs\", \"Estimasi Ukuran 8-bit\", \"Estimasi Memori Aktivasi 8-bit\"]\n",
    "param_summary_df = pd.DataFrame(summary_list)[columns_to_show]\n",
    "\n",
    "def format_flops_str(f):\n",
    "    if f is None or f == 0: return \"N/A\"\n",
    "    return f'{f / 1e9:.2f} GFLOPs' if f > 1e9 else f'{f / 1e6:.2f} MFLOPs'\n",
    "def format_bytes_str(b):\n",
    "    if b is None or b == 0: return \"N/A\"\n",
    "    return f'{b / 1e6:.2f} MB' if b > 1e6 else f'{b / 1e3:.2f} KB'\n",
    "\n",
    "param_summary_df['Total Parameter'] = param_summary_df['Total Parameter'].map('{:,}'.format)\n",
    "param_summary_df['FLOPs'] = param_summary_df['FLOPs'].map(format_flops_str)\n",
    "param_summary_df['Estimasi Ukuran 8-bit'] = param_summary_df['Estimasi Ukuran 8-bit'].map(format_bytes_str)\n",
    "param_summary_df['Estimasi Memori Aktivasi 8-bit'] = param_summary_df['Estimasi Memori Aktivasi 8-bit'].map(format_bytes_str)\n",
    "\n",
    "print(param_summary_df.to_string(index=False))\n",
    "\n",
    "# Tampilkan Rincian Arsitektur\n",
    "print(\"\\n\\n\" + \"=\"*65)\n",
    "print(f\"--- 4. Rincian Arsitektur per Model ---\")\n",
    "print(\"=\"*65)\n",
    "for model_data in summary_list:\n",
    "    print(f\"\\n>>> {model_data['Model']}:\")\n",
    "    print(model_data['Architecture Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# 6. Loop Pelatihan (Sekarang Fokus Training Saja)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "datasets = {\n",
    "    'UASpeech': (uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    'TORGO': (torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "for dataset_name, (data_files, data_labels, data_speakers) in datasets.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"MEMPROSES TRAINING DATASET: {dataset_name}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    if len(data_files) == 0: continue\n",
    "\n",
    "    # Mapping Kelas & Split (Sama seperti sebelumnya)\n",
    "    unique_classes = sorted(list(set(data_labels)))\n",
    "    class_mapping = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to Numpy for easy indexing\n",
    "    X = np.array(data_files)\n",
    "    y = np.array(data_labels)\n",
    "    groups = np.array(data_speakers)\n",
    "    \n",
    "    # 1. Split Train vs (Test + Val) - Group Aware\n",
    "    # 80% Train, 20% (Test + Val)\n",
    "    gss_outer = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, temp_idx = next(gss_outer.split(X, y, groups))\n",
    "    \n",
    "    X_train, X_temp = X[train_idx], X[temp_idx]\n",
    "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
    "    groups_train, groups_temp = groups[train_idx], groups[temp_idx]\n",
    "    \n",
    "    # 2. Split Temp (Test + Val) -> 50% Test, 50% Val (Total 10% Valid, 10% Test)\n",
    "    gss_inner = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    val_idx, test_idx = next(gss_inner.split(X_temp, y_temp, groups_temp))\n",
    "    \n",
    "    X_val, X_test = X_temp[val_idx], X_temp[test_idx]\n",
    "    y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
    "    groups_val, groups_test = groups_temp[val_idx], groups_temp[test_idx]\n",
    "    \n",
    "    # Print Distribution\n",
    "    print(f\"--- Speaker Distribution ({dataset_name}) ---\")\n",
    "    print(f\"[Train] Speakers: {sorted(list(set(groups_train)))} | Samples: {len(X_train)}\")\n",
    "    print(f\"[Val  ] Speakers: {sorted(list(set(groups_val)))}   | Samples: {len(X_val)}\")\n",
    "    print(f\"[Test ] Speakers: {sorted(list(set(groups_test)))}  | Samples: {len(X_test)}\")\n",
    "\n",
    "    for model_key, model_display_name in config.MODELS.items():\n",
    "        print(f\"\\n--- Training Pipeline: {model_display_name} @ {dataset_name} ---\")\n",
    "\n",
    "        # ... (Pipeline sama: Dataset -> Build -> Train -> Eval)\n",
    "        try:\n",
    "             # Tipe Fitur\n",
    "            feature_type = 'stft' if model_key == 'cnn_stft' else 'mfcc'\n",
    "            \n",
    "            # Create Dataset\n",
    "            train_ds = data_loader.create_tf_dataset(X_train, y_train, class_mapping, is_training=True, feature_type=feature_type)\n",
    "            val_ds = data_loader.create_tf_dataset(X_val, y_val, class_mapping, is_training=False, feature_type=feature_type)\n",
    "            test_ds = data_loader.create_tf_dataset(X_test, y_test, class_mapping, is_training=False, feature_type=feature_type)\n",
    "\n",
    "            # Get Input Shape from DS\n",
    "            input_shape = None\n",
    "            for feature, label in train_ds.take(1):\n",
    "                input_shape = feature.shape[1:]\n",
    "                break\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = models.get_model(model_key, input_shape, num_classes=len(unique_classes))\n",
    "\n",
    "            # Training\n",
    "            run_name = f\"{model_key}_{dataset_name}\"\n",
    "            history, time_taken = trainer.train_model(model, train_ds, val_ds, model_name=run_name)\n",
    "            print(f\"-> Training Done ({time_taken:.2f}s)\")\n",
    "            \n",
    "            # Eval\n",
    "            trainer.evaluate_model(model, test_ds, unique_classes, model_name=run_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Training {model_display_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "tensorboard_viz"
   },
   "outputs": [],
   "source": [
    "# 7. Visualisasi TensorBoard\n",
    "logs_base_dir = os.path.join(config.OUTPUTS_DIR, 'logs')\n",
    "%tensorboard --logdir \"{logs_base_dir}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# Write to file
with open(target_file, 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1)

print(f"Notebook created at: {target_file}")
