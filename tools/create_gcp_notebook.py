import nbformat as nbf
import json
import os
import sys

# Output File
target_file = '../notebooks/Master_Training_GCP.ipynb'
os.makedirs(os.path.dirname(target_file), exist_ok=True)

# Notebook Content Structure
notebook_content = {
 "cells": [

  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "env_setup"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Environment & Path (Auto-Detect)\n",
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# SEARCH FOR PROJECT ROOT\n",
    "# Mencari folder 'Tesis/v2.0.1' di dalam Drive\n",
    "possible_roots = [\n",
    "    '/content/drive/MyDrive/Tesis/v2.0.1',\n",
    "    '/content/drive/MyDrive/KULIAH S2/TESIS/4. Dev/Tesis/v2.0.1',\n",
    "    '/content/drive/MyDrive/4. Dev/Tesis/v2.0.1'\n",
    "]\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "for path in possible_roots:\n",
    "    if os.path.exists(path):\n",
    "        PROJECT_ROOT = path\n",
    "        break\n",
    "\n",
    "# Deep Search jika belum ketemu\n",
    "if PROJECT_ROOT is None:\n",
    "    print(\"Mencari folder v2.0.1 secara mendalam (bisa agak lama)...\")\n",
    "    import glob\n",
    "    candidates = glob.glob('/content/drive/MyDrive/**/v2.0.1', recursive=True)\n",
    "    if candidates:\n",
    "        PROJECT_ROOT = candidates[0]\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"Folder proyek 'v2.0.1' tidak ditemukan di Google Drive! Pastikan folder sudah diupload.\")\n",
    "\n",
    "print(f\"âœ… PROJECT ROOT ditemukan: {PROJECT_ROOT}\")\n",
    "\n",
    "# Setup Path\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, 'backend'))\n",
    "LOCAL_DATA_ROOT = '/content/data'\n",
    "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"Environment Siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# 2. Install Dependencies\n",
    "!pip install -q tensorflow-io\n",
    "!pip install -q pandas matplotlib seaborn scikit-learn librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# 3. Import Modul Proyek\n",
    "try:\n",
    "    from src import config, data_loader, models, trainer\n",
    "    print(\"âœ… Modul berhasil diimport: config, data_loader, models, trainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Gagal import modul: {e}\")\n",
    "    print(\"Pastikan struktur folder benar: backend/src/__init__.py ada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "data_prep_func"
   },
   "outputs": [],
   "source": [
    "# 4. Persiapan Data (Copy dari Drive ke Local VM untuk Kecepatan)\n",
    "import shutil\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "# IDs Google Drive (Backup jika file tidak ada di folder Tesis)\n",
    "UASPEECH_ID = '1L17F0SAkRk3rEjHDUyToLUvNp99sNMvE'\n",
    "TORGO_ID = '1YU7aCqa4qyn75XRdFPAWEqVv_1Qpl9cG'\n",
    "\n",
    "def setup_dataset(name, file_id, extract_path):\n",
    "    target_path = os.path.join(extract_path, name)\n",
    "\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"âœ… Dataset {name} sudah siap di {target_path}\")\n",
    "        return target_path\n",
    "\n",
    "    # 2. Cari ZIP nya\n",
    "    # Prioritas A: Cari di Google Drive (Folder Tesis/data)\n",
    "    zip_name = f\"{name}.zip\"\n",
    "    drive_zip_path = None\n",
    "    if 'DRIVE_ROOT' in globals():\n",
    "         # Cek path standar: DRIVE_ROOT/data (jika DRIVE_ROOT=Tesis/v2.0.1)\n",
    "         path1 = os.path.join(DRIVE_ROOT, 'data', zip_name)\n",
    "         path2 = os.path.join(os.path.dirname(DRIVE_ROOT), 'data', zip_name)\n",
    "         # Fallback jika DRIVE_ROOT adalah v2.0.1/backend, maka cek v2.0.1/backend/../data\n",
    "         path3 = os.path.join(os.path.dirname(DRIVE_ROOT), 'data', zip_name)\n",
    "         \n",
    "         if os.path.exists(path1):\n",
    "             drive_zip_path = path1\n",
    "         elif os.path.exists(path2):\n",
    "             drive_zip_path = path2\n",
    "         elif os.path.exists(path3):\n",
    "             drive_zip_path = path3\n",
    "         \n",
    "         if drive_zip_path:\n",
    "             print(f\"ðŸ“¦ Ditemukan Zip di Drive: {drive_zip_path}\")\n",
    "    \n",
    "    local_zip_path = os.path.join(extract_path, zip_name)\n",
    "    \n",
    "    # 3. Copy atau Download\n",
    "    if not os.path.exists(local_zip_path):\n",
    "        if drive_zip_path:\n",
    "            print(f\"Menyalin {name}.zip dari Drive ke Local VM (biar cepat)...\")\n",
    "            shutil.copy(drive_zip_path, local_zip_path)\n",
    "        else:\n",
    "            print(f\"Zip tidak ada di Drive, mendownload {name} via gdown...\")\n",
    "            url = f'https://drive.google.com/uc?id={file_id}'\n",
    "            gdown.download(url, local_zip_path, quiet=False)\n",
    "    \n",
    "    # 4. Unzip\n",
    "    print(f\"Mengekstrak {name}...\")\n",
    "    subprocess.check_call(['unzip', '-o', '-q', local_zip_path, '-d', extract_path])\n",
    "    print(f\"âœ… {name} Selesai diekstrak.\")\n",
    "\n",
    "    # Verifikasi ulang path (khusus TORGO kadang nama foldernya beda)\n",
    "    if name == 'TORGO' and not os.path.exists(target_path):\n",
    "         alt = os.path.join(extract_path, 'TORGO')\n",
    "         if os.path.exists(alt): return alt\n",
    "\n",
    "    return target_path\n",
    "\n",
    "# Jalankan Setup\n",
    "uaspeech_path = setup_dataset('UASpeech', UASPEECH_ID, LOCAL_DATA_ROOT)\n",
    "torgo_path = setup_dataset('TORGO', TORGO_ID, LOCAL_DATA_ROOT)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOADING DATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nMemuat Path File dari Disk Lokal...\")\n",
    "\n",
    "# Load Path File Audio\n",
    "uaspeech_files, uaspeech_labels, uaspeech_speakers = data_loader.get_file_paths(uaspeech_path, 'UASpeech')\n",
    "torgo_files, torgo_labels, torgo_speakers = data_loader.get_file_paths(torgo_path, 'TORGO')\n",
    "\n",
    "print(\"Path dataset berhasil dimuat. Siap untuk diproses secara terpisah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "model_analysis"
   },
   "outputs": [],
   "source": [
    "# 5. ANALISIS MODEL & PERBANDINGAN STRUKTUR (WAJIB PAPER 2)\n",
    "# Bagian ini dipisahkan agar analisa FLOPs, Parameter, dan Memory terlihat jelas sebelum Training dimulai.\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"\\n--- 2. Membangun dan Meringkas Semua Arsitektur Model ---\")\n",
    "summary_list = []\n",
    "\n",
    "# Setup Input Shape Standar untuk Analisa (4 Channel untuk Model TL, 1 Channel untuk STFT)\n",
    "# Fix: Gunakan MFCC_MAX_LEN yang benar dari config\n",
    "input_shape_mfcc = (config.N_MFCC, config.MFCC_MAX_LEN, 1)\n",
    "# Fix: Hitung N_STFT dari N_FFT/2 + 1 (Spectrogram Height)\n",
    "n_stft_bins = (config.N_FFT // 2) + 1\n",
    "input_shape_stft = (n_stft_bins, config.MFCC_MAX_LEN, 1)\n",
    "\n",
    "for model_key, model_display_name in config.MODELS.items():\n",
    "    print(f\"Menganalisis arsitektur untuk: {model_display_name}...\")\n",
    "    \n",
    "    # Tentukan input shape berdasarkan jenis model\n",
    "    current_input_shape = input_shape_stft if model_key == 'cnn_stft' else input_shape_mfcc\n",
    "    \n",
    "    # Build Model\n",
    "    tf.keras.backend.clear_session()\n",
    "    try:\n",
    "        model = models.get_model(model_key, current_input_shape, num_classes=2)\n",
    "        \n",
    "        # Hitung Metrik\n",
    "        total_params = model.count_params()\n",
    "        # Hitung FLOPs\n",
    "        flops = trainer.get_flops(model)\n",
    "        peak_mem_32bit, disk_size_32bit = trainer.get_model_memory_usage(model)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Gagal build/metric {model_display_name}: {e}\")\n",
    "        flops = 0; peak_mem_32bit = 0; disk_size_32bit = 0\n",
    "        # Dummy summary\n",
    "        architecture_summary = \"Error building model\"\n",
    "    else:\n",
    "        # Capture Summary\n",
    "        stream = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "        architecture_summary = stream.getvalue()\n",
    "        stream.close()\n",
    "\n",
    "    summary_list.append({\n",
    "        \"Model\": model_display_name,\n",
    "        \"Total Parameter\": total_params,\n",
    "        \"FLOPs\": flops,\n",
    "        \"Ukuran di Disk (32-bit)\": disk_size_32bit,\n",
    "        \"Estimasi Ukuran 8-bit\": disk_size_32bit / 4,\n",
    "        \"Estimasi Memori Aktivasi 8-bit\": peak_mem_32bit / 4,\n",
    "        \"Architecture Summary\": architecture_summary\n",
    "    })\n",
    "\n",
    "# Tampilkan Tabel Ringkasan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- 3. Tabel Ringkasan Metrik untuk Edge Device ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "columns_to_show = [\"Model\", \"Total Parameter\", \"FLOPs\", \"Estimasi Ukuran 8-bit\", \"Estimasi Memori Aktivasi 8-bit\"]\n",
    "param_summary_df = pd.DataFrame(summary_list)[columns_to_show]\n",
    "\n",
    "def format_flops_str(f):\n",
    "    if f is None or f == 0: return \"N/A\"\n",
    "    return f'{f / 1e9:.2f} GFLOPs' if f > 1e9 else f'{f / 1e6:.2f} MFLOPs'\n",
    "def format_bytes_str(b):\n",
    "    if b is None or b == 0: return \"N/A\"\n",
    "    return f'{b / 1e6:.2f} MB' if b > 1e6 else f'{b / 1e3:.2f} KB'\n",
    "\n",
    "param_summary_df['Total Parameter'] = param_summary_df['Total Parameter'].map('{:,}'.format)\n",
    "param_summary_df['FLOPs'] = param_summary_df['FLOPs'].map(format_flops_str)\n",
    "param_summary_df['Estimasi Ukuran 8-bit'] = param_summary_df['Estimasi Ukuran 8-bit'].map(format_bytes_str)\n",
    "param_summary_df['Estimasi Memori Aktivasi 8-bit'] = param_summary_df['Estimasi Memori Aktivasi 8-bit'].map(format_bytes_str)\n",
    "\n",
    "print(param_summary_df.to_string(index=False))\n",
    "\n",
    "# Tampilkan Rincian Arsitektur\n",
    "print(\"\\n\\n\" + \"=\"*65)\n",
    "print(f\"--- 4. Rincian Arsitektur per Model ---\")\n",
    "print(\"=\"*65)\n",
    "for model_data in summary_list:\n",
    "    print(f\"\\n>>> {model_data['Model']}:\")\n",
    "    print(model_data['Architecture Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# 6. Loop Pelatihan (Sekarang Fokus Training Saja)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "datasets = {\n",
    "    'UASpeech': (uaspeech_files, uaspeech_labels, uaspeech_speakers),\n",
    "    'TORGO': (torgo_files, torgo_labels, torgo_speakers)\n",
    "}\n",
    "\n",
    "for dataset_name, (data_files, data_labels, data_speakers) in datasets.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"MEMPROSES TRAINING DATASET: {dataset_name}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    if len(data_files) == 0: continue\n",
    "\n",
    "    # Mapping Kelas & Split (Sama seperti sebelumnya)\n",
    "    unique_classes = sorted(list(set(data_labels)))\n",
    "    class_mapping = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to Numpy for easy indexing\n",
    "    X = np.array(data_files)\n",
    "    y = np.array(data_labels)\n",
    "    groups = np.array(data_speakers)\n",
    "    \n",
    "    # 1. Split Train vs (Test + Val) - Group Aware\n",
    "    # 80% Train, 20% (Test + Val)\n",
    "    gss_outer = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, temp_idx = next(gss_outer.split(X, y, groups))\n",
    "    \n",
    "    X_train, X_temp = X[train_idx], X[temp_idx]\n",
    "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
    "    groups_train, groups_temp = groups[train_idx], groups[temp_idx]\n",
    "    \n",
    "    # 2. Split Temp (Test + Val) -> 50% Test, 50% Val (Total 10% Valid, 10% Test)\n",
    "    gss_inner = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    val_idx, test_idx = next(gss_inner.split(X_temp, y_temp, groups_temp))\n",
    "    \n",
    "    X_val, X_test = X_temp[val_idx], X_temp[test_idx]\n",
    "    y_val, y_test = y_temp[val_idx], y_temp[test_idx]\n",
    "    groups_val, groups_test = groups_temp[val_idx], groups_temp[test_idx]\n",
    "    \n",
    "    # Print Distribution\n",
    "    print(f\"--- Speaker Distribution ({dataset_name}) ---\")\n",
    "    print(f\"[Train] Speakers: {sorted(list(set(groups_train)))} | Samples: {len(X_train)}\")\n",
    "    print(f\"[Val  ] Speakers: {sorted(list(set(groups_val)))}   | Samples: {len(X_val)}\")\n",
    "    print(f\"[Test ] Speakers: {sorted(list(set(groups_test)))}  | Samples: {len(X_test)}\")\n",
    "\n",
    "    for model_key, model_display_name in config.MODELS.items():\n",
    "        print(f\"\\n--- Training Pipeline: {model_display_name} @ {dataset_name} ---\")\n",
    "\n",
    "        # ... (Pipeline sama: Dataset -> Build -> Train -> Eval)\n",
    "        try:\n",
    "             # Tipe Fitur\n",
    "            feature_type = 'stft' if model_key == 'cnn_stft' else 'mfcc'\n",
    "            \n",
    "            # Create Dataset\n",
    "            train_ds = data_loader.create_tf_dataset(X_train, y_train, class_mapping, is_training=True, feature_type=feature_type)\n",
    "            val_ds = data_loader.create_tf_dataset(X_val, y_val, class_mapping, is_training=False, feature_type=feature_type)\n",
    "            test_ds = data_loader.create_tf_dataset(X_test, y_test, class_mapping, is_training=False, feature_type=feature_type)\n",
    "\n",
    "            # Get Input Shape from DS\n",
    "            input_shape = None\n",
    "            for feature, label in train_ds.take(1):\n",
    "                input_shape = feature.shape[1:]\n",
    "                break\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = models.get_model(model_key, input_shape, num_classes=len(unique_classes))\n",
    "\n",
    "            # Training\n",
    "            run_name = f\"{model_key}_{dataset_name}\"\n",
    "            history, time_taken = trainer.train_model(model, train_ds, val_ds, model_name=run_name)\n",
    "            print(f\"-> Training Done ({time_taken:.2f}s)\")\n",
    "            \n",
    "            # Eval\n",
    "            trainer.evaluate_model(model, test_ds, unique_classes, model_name=run_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR Training {model_display_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "id": "tensorboard_viz"
   },
   "outputs": [],
   "source": [
    "# 7. Visualisasi TensorBoard\n",
    "logs_base_dir = os.path.join(config.OUTPUTS_DIR, 'logs')\n",
    "%tensorboard --logdir \"{logs_base_dir}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# Write to file
with open(target_file, 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1)

print(f"Notebook created at: {target_file}")
